snippet abs "abs"
abs(x)

endsnippet

snippet built_in.abs=>int "built_in.abs=>int"
x = abs(-7.25)
print(x)

endsnippet

snippet built_in.abs=>float "built_in.abs=>float"
x = abs(-20)
print(x)

endsnippet

snippet built_in.abs=>complex "built_in.abs=>complex"
x = abs((3 - 4j))
print(x)

endsnippet

snippet all "all"
all(iterable)

endsnippet

snippet built_in.all=>list_1 "built_in.all=>list_1"
mylist = [True, True, True]
x = all(mylist)
print(x)

# Return True

endsnippet

snippet built_in.all=>list_2 "built_in.all=>list_2"
mylist = [0, 1, 1]
x = all(mylist)
print(x)

# Returns False because 0 is the same as False

endsnippet

snippet built_in.all=>tuple "built_in.all=>tuple"
mytuple = (0, True, False)
x = all(mytuple)
print(x)

# Returns False because both the first and the third items are False

endsnippet

snippet built_in.all=>set "built_in.all=>set"
myset = {0, 1, 0}
x = all(myset)
print(x)

# Returns False because both the first and the third items are False

endsnippet

snippet built_in.all=>dictionary "built_in.all=>dictionary"
mydict = {0 : 'Apple', 1 : 'Orange'}
x = all(mydict)
print(x)

# Returns False because the first key is false.
# For dictionaries the all() function checks the keys, not the values.

endsnippet

snippet any "any"
any(iterable)

endsnippet

snippet built_in.any=>list_1 "built_in.any=>list_1"
mylist = [False, True, False]
x = any(mylist)
print(x)

# Return True

endsnippet

snippet ascii "ascii"
ascii(object)

endsnippet

snippet built_in.ascii=>_list_1 "built_in.ascii=>_1"
x = ascii('My name is StÃ¥le')
print(x)

endsnippet

snippet bin "bin"
bin(x)

endsnippet

snippet built_in.bin=>_1 "built_in.bin=>_1"
x = bin(36)
print(x)
# Result : 0b100100

endsnippet

snippet bool "bool"
bool(object)

endsnippet

snippet built_in.bool=>_1 "built_in.bool=>_1"
x = bool(1)
print(x)
# Result : True

endsnippet

snippet bytearray "bytearray"
bytearray([source[, encoding[, errors]]])

endsnippet

snippet built_in.bytearray=>_1 "built_in.bytearray=>_1"
x = bytearray(4)
print(x)

endsnippet

snippet bytes "bytes"
bytes(x, encoding, error)

endsnippet

snippet built_in.bytes=>_1 "built_in.bytes=>_1"
x = x = bytes(4)
print(x)

endsnippet

snippet callable "callable"
callable(object)

endsnippet

snippet built_in.callable=>_1 "built_in.callable=>_1"
def x():
a = 5

print(callable(x))

endsnippet

snippet built_in.callable=>_2 "built_in.callable=>_2"
x = 5

print(callable(x))

endsnippet

snippet chr "chr"
chr(i)

endsnippet

snippet built_in.chr=>_1 "built_in.chr=>_1"
x = chr(97)

print(x)

endsnippet

snippet classmethod "classmethod"
classmethod(function)

endsnippet

snippet compile "compile"
compile(source, filename, mode, flag, dont_inherit, optimize)

endsnippet

snippet built_in.compile=>_1 "built_in.compile=>_1"
mytext = 'print(55)'
x = compile('mytext', 'test', 'eval')
exec(x)

endsnippet

snippet built_in.compile=>_1 "built_in.compile=>_2"
mytext = 'print(55)
print(88)'
x = compile('mytext', 'test', 'exec')
exec(x)

endsnippet

snippet complex "complex"
complex(real, imaginary)

endsnippet

snippet built_in.complex=>_1 "built_in.complex=>_1"
x = complex(3, 5)
print(x)

endsnippet

snippet built_in.complex=>_2 "built_in.complex=>_2"
x = complex('3+5j')
print(x)

endsnippet

snippet delattr "delattr"
delattr(object, attribute)

endsnippet

snippet built_in.delattr=>_1 "built_in.delattr=>_1"
class Person:
 name = 'John'
 age = 36
 country = 'Norway'

delattr(Person, 'age')
# The Person object will no longer contain an age property

endsnippet

snippet dict "dict"
dict(keyword arguments)

endsnippet

snippet built_in.dict=>_1 "built_in.dict=>_1"
x = dict(name = 'John', age = 36, country = 'Norway')
print(x)

endsnippet

snippet dir "dir"
dir(object)

endsnippet

snippet built_in.dir=> "built_in.dir=>"
class Person:
 name = 'John'
 age = 36
 country = 'Norway'
print(dir(Person))

endsnippet

snippet divmod "divmod"
divmod(divident, divisor)

endsnippet

snippet built_in.divmod=>_1 "built_in.divmod=>_1"
x = divmod(5, 2)
print(x)

endsnippet

snippet enumerate "enumerate"
enumerate(iterable, start)

endsnippet

snippet built_in.enumerate=>_1 "built_in.enumerate=>_1"
x = ('apple', 'banana', 'cherry')
y = enumerate(x)

print(list(y))

endsnippet

snippet eval "eval"
eval(expression, globals, locals)

endsnippet

snippet built_in.eval=>_1 "built_in.eval=>_1"
x = 'print(55)'
eval(x)

endsnippet

snippet exec "exec"
exec(object, globals, locals)

endsnippet

snippet built_in.exec=>_1 "built_in.exec=>_1"
x = 'age = 25
print(age)'
exec(x)

endsnippet

snippet filter "filter"
filter(function, iterable)

endsnippet

snippet built_in.filter=>_1 "built_in.filter=>_1"
ages = [5, 12, 17, 18, 24, 32]

def myFunc(x):
 if x < 18:
   return False
 else:
   return True

adults = filter(myFunc, ages)

for x in adults:
 print(x)

endsnippet

snippet float "float"
float(value)

endsnippet

snippet built_in.float=>_1 "built_in.float=>_1"
x = float(3)
print(x)

endsnippet

snippet built_in.float=>_2 "built_in.float=>_2"
x = float('3.500')
print(x)

endsnippet

snippet .format "format"
.format()

endsnippet

snippet built_in.format=>_1 "built_in.format=>_1"
x = format(255, 'x')
print(x)

endsnippet

snippet frozenset "frozenset"
frozenset(iterable)

endsnippet

snippet built_in.frozenset=>_1 "built_in.frozenset=>_1"
mylist = ['apple', 'banana', 'cherry']
x = frozenset(mylist)
print(x)

endsnippet

snippet built_in.frozenset=>_2 "built_in.frozenset=>_2"
mylist = ['apple', 'banana', 'cherry']
x = frozenset(mylist)
x[1] = 'strawberry'
print(x)

endsnippet

snippet getattr "getattr"
getattr(object, attribute, default)

endsnippet

snippet built_in.frozenset=>_3 "built_in.frozenset=>_3"
class Person:
 name = 'John'
 age = 36
 country = 'Norway'

x = getattr(Person, 'age')

print(x)

endsnippet

snippet globals "globals"
globals()

endsnippet

snippet built_in.globals=>_1 "built_in.globals=>_1"
x = globals()
print(x)

endsnippet

snippet built_in.globals=>_2 "built_in.globals=>_2"
x = globals()
print(x['__file__'])

endsnippet

snippet hasattr "hasattr"
hasattr(object, attribute)

endsnippet

snippet built_in.hasattr=> "built_in.hasattr=>"
class Person:
 name = 'John'
 age = 36
 country = 'Norway'

x = hasattr(Person, 'age')

print(x)

endsnippet

snippet hash "hash"
hash(object)

endsnippet

snippet help "help"
help(object)

endsnippet

snippet hex "hex"
hex(number)

endsnippet

snippet built_in.hex=> "built_in.hex=>"
x = hex(255)
print(x)

endsnippet

snippet int "int"
int(value, base)

endsnippet

snippet built_in.int=>_1 "built_in.int=>_1"
x = int(3.5)
print(x)

endsnippet

snippet built_in.int=>_2 "built_in.int=>_2"
x = int('12')
print(x)

endsnippet

snippet id "id"
id(object)

endsnippet

snippet built_in.id=> "built_in.id=>"
class Foo:
b = 5

dummyFoo = Foo()
print('id of dummyFoo =',id(dummyFoo))

endsnippet

snippet input "input"
input(prompt)

endsnippet

snippet built_in.input=>_1 "built_in.input=>_1"
x = input('Enter your name:')
print('Hello, ' + x)

endsnippet

snippet built_in.input=>_2 "built_in.input=>_2"
print('Enter your name:')
x = input()
print('Hello, ' + x)

endsnippet

snippet isinstance "isinstance"
isinstance(object, type)

endsnippet

snippet built_in.isinstance=>_1 "built_in.isinstance=>_1"
x = isinstance(5, int)

print(x)

endsnippet

snippet built_in.isinstance=>_2 "built_in.isinstance=>_2"
x = isinstance('Hello', (float, int, str, list, dict, tuple))

print(x)

endsnippet

snippet built_in.isinstance=>_3 "built_in.isinstance=>_3"
class myObj:
 name = 'John'

y = myObj()

x = isinstance(y, myObj)
print(x)

endsnippet

snippet issubclass "issubclass"
issubclass(object, subclass)

endsnippet

snippet built_in.issubclass=> "built_in.issubclass=>"
class myAge:
 age = 36

class myObj(myAge):
 name = 'John'
 age = myAge

 x = issubclass(myObj, myAge)

print(x)

endsnippet

snippet iter "iter"
iter(object, subclass)

endsnippet

snippet built_in.iter=> "built_in.iter=>"
x = iter(['apple', 'banana', 'cherry'])
print(next(x))
print(next(x))
print(next(x))

endsnippet

snippet len "len"
len(s)

endsnippet

snippet built_in.len=>_1 "built_in.len=>_1"
mylist = ['apple', 'banana', 'cherry']
x = len(mylist)

endsnippet

snippet built_in.len=>_2 "built_in.len=>_2"
mylist = 'Hello'
x = len(mylist)

endsnippet

snippet list "list"
list([iterable])

endsnippet

snippet built_in.list=> "built_in.list=>"
x = list(('apple', 'banana', 'cherry'))
print(x)

endsnippet

snippet locals "locals"
locals()

endsnippet

snippet built_in.locals=>_1 "built_in.locals=>_1"
x = locals()
print(x)

endsnippet

snippet built_in.locals=>_2 "built_in.locals=>_2"
x = locals()
print(x['__file__'])

endsnippet

snippet map "map"
map(function, iterables)

endsnippet

snippet built_in.map=>_1 "built_in.map=>_1"
def myfunc(n):
 return len(n)

x = map(myfunc, ('apple', 'banana', 'cherry'))

print(x)

endsnippet

snippet built_in.map=>_2 "built_in.map=>_2"
def myfunc(a, b):
 return a + b

x = map(myfunc, ('apple', 'banana', 'cherry'), ('orange', 'lemon', 'pineapple'))

print(x)

endsnippet

snippet max "max"
max(iterable)

endsnippet

snippet built_in.max=>_1 "built_in.max=>_1"
x = max(5, 10)
print(x)

endsnippet

snippet built_in.max=>_2 "built_in.max=>_2"
x = max('Mike', 'John', 'Vicky')
print(x)

endsnippet

snippet built_in.max=>_3 "built_in.max=>_3"
a = (1, 5, 3, 9)
x = max(a)
print(x)

endsnippet

snippet memoryview "memoryview"
memoryview(obj)

endsnippet

snippet built_in.memoryview=> "built_in.memoryview=>"
x = memoryview(b'Hello')
print(x)

#return the Unicode of the first character
print(x[0])

#return the Unicode of the second character
print(x[1])

endsnippet

snippet min "min"
min(iterable)

endsnippet

snippet built_in.min=>_1 "built_in.min=>_1"
x = min(5, 10)
print(x)

endsnippet

snippet built_in.min=>_2 "built_in.min=>_2"
x = min('Mike', 'John', 'Vicky')
print(x)

endsnippet

snippet built_in.min=>_3 "built_in.min=>_3"
a = (1, 5, 3, 9)
x = min(a)
print(x)

endsnippet

snippet next "next"
next(iterable, default)

endsnippet

snippet built_in.next=>_1 "built_in.next=>_1"
mylist = iter(['apple', 'banana', 'cherry'])
x = next(mylist)
print(x)
x = next(mylist)
print(x)
x = next(mylist)
print(x)

endsnippet

snippet built_in.next=>_2 "built_in.next=>_2"
mylist = iter(['apple', 'banana', 'cherry'])
x = next(mylist, 'orange')
print(x)
x = next(mylist, 'orange')
print(x)
x = next(mylist, 'orange')
print(x)
x = next(mylist, 'orange')
print(x)

endsnippet

snippet object "object"
object()

endsnippet

snippet built_in.object=> "built_in.object=>"
x = object()
print(dir(x))

endsnippet

snippet oct "oct"
oct(x)

endsnippet

snippet built_in.oct=> "built_in.oct=>"
x = oct(12)
print(x)

endsnippet

snippet open "open"
open(file, mode)

endsnippet

snippet built_in.open=> "built_in.open=>"
f = open('demofile.txt', 'r')
print(f.read())

endsnippet

snippet ord "ord"
ord(c)

endsnippet

snippet built_in.ord=> "built_in.ord=>"
x=ord('a')
print(x)

endsnippet

snippet pow "pow"
pow(x, y)

endsnippet

snippet built_in.pow=> "built_in.pow=>"
x=pow(2,5)
print(x)

endsnippet

snippet print "print"
print(object(s), separator=separator, end=end, file=file, flush=flush)

endsnippet

snippet built_in.print=>_1 "built_in.print=>_1"
print('Hello', 'how are you?')

endsnippet

snippet built_in.print=>_2 "built_in.print=>_2"
x = ('apple', 'banana', 'cherry')
print(x)

endsnippet

snippet built_in.print=>_3 "built_in.print=>_3"
print('Hello', 'how are you?', sep=' ---')

endsnippet

snippet property "property"
property(fget=None, fset=None, fdel=None, doc=None)

endsnippet

snippet built_in.property=> "built_in.property=>"
class C:
    def __init__(self):
        self._x = None
    def getx(self):
        return self._x
    def setx(self, value):
        self._x = value
    def delx(self):
        del self._x
    x = property(getx, setx, delx, 'I'm the 'x' property.')

endsnippet

snippet range "range"
range(start, stop, step)

endsnippet

snippet built_in.range=>_1 "built_in.range=>_1"
x = range(6)
for n in x:
  print(n)

endsnippet

snippet built_in.range=>_2 "built_in.range=>_2"
x = range(3, 6)
for n in x:
  print(n)

endsnippet

snippet built_in.range=>_3 "built_in.range=>_3"
x = range(3, 20, 2)
for n in x:
  print(n)

endsnippet

snippet repr "repr"
repr(object)

endsnippet

snippet reversed "reversed"
reversed(seq)

endsnippet

snippet built_in.reversed=> "built_in.reversed=>"
alph = ['a', 'b', 'c', 'd']
ralph = reversed(alph)
for x in ralph:
  print(x)

endsnippet

snippet round "round"
round(number[, ndigits])

endsnippet

snippet built_in.round=>_1 "built_in.round=>_1"
x = round(5.76543, 2)
print(x)

endsnippet

snippet built_in.round=>_2 "built_in.round=>_2"
x = round(5.76543)
print(x)

endsnippet

snippet set "set"
set(iterable)

endsnippet

snippet built_in.set=> "built_in.set=>"
x = set(('apple', 'banana', 'cherry'))
print(x)

endsnippet

snippet setattr "setattr"
setattr(object, name, value)

endsnippet

snippet built_in.setattr=> "built_in.setattr=>"
class Person:
  name = 'John'
  age = 36
  country = 'Norway'
setattr(Person, 'age', 40)
# The age property will now have the value: 40
x = getattr(Person, 'age')
print(x)

endsnippet

snippet slice "slice"
slice(start, end, step)

endsnippet

snippet built_in.slice=>_1 "built_in.slice=>_1"
a = ('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
x = slice(2)
print(a[x])

endsnippet

snippet built_in.slice=>_2 "built_in.slice=>_2"
a = ('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
x = slice(3, 5)
print(a[x])

endsnippet

snippet built_in.slice=>_3 "built_in.slice=>_3"
a = ('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
x = slice(0, 8, 3)
print(a[x])

endsnippet

snippet sorted "sorted"
sorted(iterable, key=key, reverse=reverse)

endsnippet

snippet built_in.sorted=>_1 "built_in.sorted=>_1"
a = ('b', 'g', 'a', 'd', 'f', 'c', 'h', 'e')
x = sorted(a)
print(x)

endsnippet

snippet built_in.sorted=>_2 "built_in.sorted=>_2"
a = ('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
x = sorted(a, reverse=True)
print(x)

endsnippet

snippet built_in.sorted=>_3 "built_in.sorted=>_3"
a = ('b', 'g', 'a', 'd', 'f', 'c', 'h', 'e')
x = sorted(a)
print(x)

endsnippet

snippet staticmethod "staticmethod"
staticmethod(function)

endsnippet

snippet str "str"
str(object, encoding=encoding, errors=errors)

endsnippet

snippet built_in.str=> "built_in.str=>"
x = str(3.5)
print(x)

endsnippet

snippet sum "sum"
sum(iterable, start)

endsnippet

snippet built_in.sum=>_1 "built_in.sum=>_1"
a = (1, 2, 3, 4, 5)
x = sum(a)
print(x)

endsnippet

snippet built_in.sum=>_2 "built_in.sum=>_2"
a = (1, 2, 3, 4, 5)
x = sum(a, 7)
print(x)

endsnippet

snippet super "super"
super(type[, object-or-type])

endsnippet

snippet tuple "tuple"
tuple(iterable)

endsnippet

snippet built_in.tuple=> "built_in.tuple=>"
x = tuple(('apple', 'banana', 'cherry'))
print(x)

endsnippet

snippet type "type"
type(object, bases, dict)

endsnippet

snippet built_in.type=> "built_in.type=>"
a = ('apple', 'banana', 'cherry')
b = 'Hello World'
c = 33
x = type(a)
y = type(b)
z = type(c)

endsnippet

snippet unichr "unichr"
unichr(i)

endsnippet

snippet vars "vars"
vars(object)

endsnippet

snippet built_in.vars=> "built_in.vars=>"
class Person:
  name = 'John'
  age = 36
  country = 'norway'
x = vars(Person)

endsnippet

snippet zip "zip"
zip(iterator1, iterqator2, iterator3 ...)

endsnippet

snippet built_in.zip=>_1 "built_in.zip=>_1"
a = ('John', 'Charles', 'Mike')
b = ('Jenny', 'Christy', 'Monica')
x = zip(a, b)
#use the tuple() function to display a readable version of the result:
print(tuple(x))

endsnippet

snippet built_in.zip=>_2 "built_in.zip=>_2"
a = ('John', 'Charles', 'Mike')
b = ('Jenny', 'Christy', 'Monica', 'Vicky')
x = zip(a, b)
#use the tuple() function to display a readable version of the result:
print(tuple(x))

endsnippet

snippet if "if"
if condition:
  pass

endsnippet

snippet ifelif "ifelif"
if condition:
  pass
elif condition:
  pass

endsnippet

snippet ifelifelse "ifelifelse"
if condition:
  pass
elif condition:
  pass
else:
  pass

endsnippet

snippet ifelse "ifel"
if condition:
  pass
else:
  pass

endsnippet

snippet else "elif"
else:
  pass

endsnippet

snippet ifshort "ifshort"
print('A') if a > b else print('A')

endsnippet

snippet lambda "lambda"
lambda arguments : expression

endsnippet

snippet for "for"
for item in range:


endsnippet

snippet for=> "for=>"
fruits = ['apple', 'banana', 'cherry']
for x in fruits:
  print(x)

endsnippet

snippet for=>through_a_string "for=>through_a_string"
for x in 'banana':
  print(x)

endsnippet

snippet for=>break_statement "for=>break_statement"
fruits = ['apple', 'banana', 'cherry']
for x in fruits:
 print(x)
 if x == 'banana':
   break

endsnippet

snippet for=>continue_statement "for=>continue_statement"
fruits = ['apple', 'banana', 'cherry']
for x in fruits:
 print(x)
 if x == 'banana':
   continue
 print(x)

endsnippet

snippet for=>range_function_1 "for=>range_function_1"
for x in range(6):
 print(x)

endsnippet

snippet for=>range_function_2 "for=>range_function_2"
for x in range(2, 6):
 print(x)

endsnippet

snippet for=>range_function_3 "for=>range_function_3"
for x in range(2, 30, 3):
 print(x)

endsnippet

snippet for=>for_else "for=>for_else"
for x in range(2, 6):
 print(x)
else:
 print('Finally finished!')

endsnippet

snippet for=>for_else "for=>nested_loops"
adj = ['red', 'big', 'tasty']
fruits = ['apple', 'banana', 'cherry']
for x in adj:
  for y in fruits:
    print(x, y)

endsnippet

snippet while "while"
while expression:
  pass

endsnippet

snippet while_else "while_else"
while expression:
  pass
else:
  pass

endsnippet

snippet while=> "while=>"
i = 1
while i < 6:
  print(i)
  i += 1

endsnippet

snippet while=>break_statement "while=>break_statement"
i = 1
while i < 6:
 print(i)
  if i == 3:
    break
  i += 1

endsnippet

snippet while=>continue_statement "while=>continue_statement"
i = 1
while i < 6:
  i += 1
  print(i)
  if i == 3:
    continue
  print(i)

endsnippet

snippet function=> "function"
def name(args):
 pass

endsnippet

snippet def=> "def"
def name(args):
 pass

endsnippet

snippet def=>with_default_value "def=>with_default_value"
def name(name, lastName='john')
 pass

endsnippet

snippet .capitalize "capitalize"
.capitalize()

endsnippet

snippet string.capitalize=>_1 "string.capitalize=>_1"
txt = hello, and welcome to my world.

x = txt.capitalize()

print (x)

endsnippet

snippet string.capitalize=>_2 "string.capitalize=>_2"
txt = '36 is my age.'

x = txt.capitalize()

print (x)

endsnippet

snippet .casefold "casefold"
.casefold()

endsnippet

snippet string.casefold=> "string.casefold=>"
txt = 'Hello, And Welcome To My World!'

x = txt.casefold()

print(x)

endsnippet

snippet .center "center"
.center()

endsnippet

snippet string.center=>_1 "string.center=>_1"
txt = 'banana'

x = txt.center(20)

print(x)

endsnippet

snippet string.center=>_2 "string.center=>_2"
txt = 'banana'

x = txt.center(20,'O')

print(x)

endsnippet

snippet .count "string.count"
.count()

endsnippet

snippet string.count=>_1 "string.count=>_1"
txt = 'I love apples, apple are my favorite fruit'

x = txt.count('apple')

print(x)

endsnippet

snippet string.count=>_2 "string.count=>_2"
txt = 'I love apples, apple are my favorite fruit'

x = txt.count('apple', 10, 24)

print(x)

endsnippet

snippet .encode "encode"
.encode()

endsnippet

snippet string.encode=> "string.encode=>"
txt = 'My name is StÃ¥le'

print(txt.encode(encoding='ascii',errors='backslashreplace')
print(txt.encode(encoding='ascii',errors='ignore')
print(txt.encode(encoding='ascii',errors='namereplace')
print(txt.encode(encoding='ascii',errors='replace')
print(txt.encode(encoding='ascii',errors='xmlcharrefreplace')
print(txt.encode(encoding='ascii',errors='strict')

endsnippet

snippet .endswith "endswith"
.endswith()

endsnippet

snippet string.endswith=>_1 "string.endswith=>_1"
txt = 'Hello, And Welcome To My World!'

x = txt.endswith('.')

print(x)

endsnippet

snippet string.endswith=>_2 "string.endswith=>_2"
txt = 'Hello, And Welcome To My World!'

x = txt.endswith('my world.', 5, 11)

print(x)

endsnippet

snippet .expandtabs "expandtabs"
.expandtabs()

endsnippet

snippet string.expandtabs=>_1 "string.expandtabs=>_1"
txt = 'H	e	l	l	o'

x = txt.expandtabs(2)

print(x)

endsnippet

snippet string.expandtabs=>_2 "string.expandtabs=>_2"
txt = 'H	e	l	l	o'

print(txt)
print(txt.expandtabs())
print(txt.expandtabs(2))
print(txt.expandtabs(4))
print(txt.expandtabs(10))

endsnippet

snippet .find "find"
.find()

endsnippet

snippet string.find=>_1 "string.find=>_1"
txt = 'Hello, welcome to my world.'

x = txt.find('welcome')

print(x)

endsnippet

snippet string.find=>_2 "string.find=>_2"
txt = 'Hello, And Welcome To My World!'

x = txt.find('e')

print(x)

endsnippet

snippet string.find=>_3 "string.find=>_3"
txt = 'Hello, And Welcome To My World!'

x = txt.find('e', 5, 10)

print(x)

endsnippet

snippet string.find=>_4 "string.find=>_4"
txt = 'Hello, And Welcome To My World!'

print(txt.find('q'))
print(txt.index('q'))

endsnippet

snippet string.format=> "string.format=>"
# default arguments
print('Hello {}, your balance is {}.'.format('Adam', 230.2346))

# positional arguments
print('Hello {0}, your balance is {1}.'.format('Adam', 230.2346))

# keyword arguments
print('Hello {name}, your balance is {blc}.'.format(name='Adam', blc=230.2346))

# mixed arguments
print('Hello {0}, your balance is {blc}.'.format('Adam', blc=230.2346))

endsnippet

snippet .format_map "format_map"
.format_map()

endsnippet

snippet string.format_map=> "string.format_map=>"
point = {'x':4,'y':-5}
print('{x} {y}'.format_map(point))

point = {'x':4,'y':-5, 'z': 0}
print('{x} {y} {z}'.format_map(point))

endsnippet

snippet .index "index"
.index(value)

endsnippet

snippet string.index=>_1 "string.index=>_1"
txt = 'Hello, welcome to my world.'

x = txt.index('welcome')

print(x)

endsnippet

snippet string.index=>_2 "string.index=>_2"
txt = 'Hello, And Welcome To My World!'

x = txt.index('e')

print(x)

endsnippet

snippet string.index=>_3 "string.index=>_3"
txt = 'Hello, And Welcome To My World!'

x = txt.index('e', 5, 10)

print(x)

endsnippet

snippet string.index=>_4 "string.index=>_4"
txt = 'Hello, And Welcome To My World!'

print(txt.find('q'))
print(txt.index('q'))

endsnippet

snippet .isalnum "isalnum"
.isalnum()

endsnippet

snippet string.isalnum=> "string.isalnum=>"
txt = 'Company 12'
x = txt.isalnum()
print(x)

endsnippet

snippet .isalpha "isalpha"
.isalpha()

endsnippet

snippet string.isalpha=> "string.isalpha=>"
txt = 'Company10'
x = txt.isalpha()
print(x)

endsnippet

snippet .isdecimal "isdecimal"
.isdecimal()

endsnippet

snippet string.isdecimal=> "string.isdecimal=>"
a = '0' #unicode for 0
b = 'G' #unicode for G
print(a.isdecimal())
print(b.isdecimal())

endsnippet

snippet .isdigit "isdigit"
.isdigit()

endsnippet

snippet string.isdigit=> "string.isdigit=>"
a = '0' #unicode for 0
b = 'Â²' #unicode for Â²
print(a.isdigit())
print(b.isdigit())

endsnippet

snippet .isidentifier "isidentifier"
.isidentifier()

endsnippet

snippet string.isidentifier=> "string.isidentifier=>"
a = 'MyFolder'
b = 'Demo002'
c = '2bring'
d = 'my demo'
print(a.isidentifier())
print(b.isidentifier())
print(c.isidentifier())
print(d.isidentifier())

endsnippet

snippet .islower "islower"
.islower()

endsnippet

snippet string.islower=> "string.islower=>"
a = 'Hello world!'
b = 'hello 123'
c = 'mynameisPeter'
print(a.islower())
print(b.islower())
print(c.islower())

endsnippet

snippet .isnumeric "isnumeric"
.isnumeric()

endsnippet

snippet string.isnumeric=> "string.isnumeric=>"
a = '0' #unicode for 0
b = 'Â²' #unicode for &sup2;
c = '10km2'
print(a.isnumeric())
print(b.isnumeric())
print(c.isnumeric())

endsnippet

snippet .isprintable "isprintable"
.isprintable()

endsnippet

snippet string.isprintable=> "string.isprintable=>"
txt = 'Hello!
Are you #1?'
x = txt.isprintable()
print(x)

endsnippet

snippet .isspace "isspace"
.isspace()

endsnippet

snippet string.isspace=> "string.isspace=>"
txt = '   s   '
x = txt.isspace()
print(x)

endsnippet

snippet .istitle "istitle"
.istitle()

endsnippet

snippet string.istitle=> "string.istitle=>"
a = 'HELLO, AND WELCOME TO MY WORLD'
b = 'Hello'
c = '22 Names'
d = 'This Is %'!?'
print(a.istitle())
print(b.istitle())
print(c.istitle())
print(d.istitle())

endsnippet

snippet .isupper "isupper"
.isupper()

endsnippet

snippet string.isupper=> "string.isupper=>"
a = 'Hello World!'
b = 'hello 123'
c = 'MY NAME IS PETER'
print(a.isupper())
print(b.isupper())
print(c.isupper())

endsnippet

snippet .join "join"
.join()

endsnippet

snippet string.join=> "string.join=>"
myDict = {'name': 'John', 'country': 'Norway'}
mySeparator = 'TEST'
x = mySeparator.join(myDict)
print(x)

endsnippet

snippet .ljust "ljust"
.ljust()

endsnippet

snippet string.ljust=> "string.ljust=>"
txt = 'banana'
x = txt.ljust(20, 'O')
print(x)

endsnippet

snippet .lower "lower"
.lower()

endsnippet

snippet string.lower=> "string.lower=>"
txt = 'Hello my FRIENDS'
x = txt.lower()
print(x)

endsnippet

snippet .lstrip "lstrip"
.lstrip()

endsnippet

snippet string.lstrip=> "string.lstrip=>"
txt = ',,,,,ssaaww.....banana'
x = txt.lstrip(',.asw')
print(x)

endsnippet

snippet .maketrans "maketrans"
.maketrans()

endsnippet

snippet string.maketrans=> "string.maketrans=>"
# example dictionary
dict = {97: '123', 98: '456', 99: '789'}
string = 'abc'
print(string.maketrans(dict))

endsnippet

snippet .partition "partition"
.partition()

endsnippet

snippet string.partition=> "string.partition=>"
txt = 'I could eat bananas all day'
x = txt.partition('apples')
print(x)

endsnippet

snippet .replace "replace"
.replace(x, y)

endsnippet

snippet string.replace=> "string.replace=>"
txt = 'one one was a race horse, two two was one too.'
x = txt.replace('one', 'three', 2)
print(x)

endsnippet

snippet .rfind "rfind"
.rfind()

endsnippet

snippet string.rfind=> "string.rfind=>"
txt = 'Hello, welcome to my world.'
print(txt.rfind('q'))
print(txt.rindex('q'))

endsnippet

snippet .rindex "rindex"
.rindex()

endsnippet

snippet string.rindex=> "string.rindex=>"
txt = 'Hello, welcome to my world.'
print(txt.rfind('q'))
print(txt.rindex('q'))

endsnippet

snippet .rpartition "rpartition"
.rpartition()

endsnippet

snippet string.rpartition=> "string.rpartition=>"
txt = 'I could eat bananas all day, bananas are my favorite fruit'
x = txt.rpartition('apples')
print(x)

endsnippet

snippet .rsplit "rsplit"
.rsplit()

endsnippet

snippet string.rsplit=> "string.rsplit=>"
txt = 'banana,,,,,ssaaww.....'
x = txt.rstrip(',.asw')
print(x)

endsnippet

snippet .split "split"
.split()

endsnippet

snippet string.split=> "string.split=>"
txt = 'apple#banana#cherry#orange'
# setting the max parameter to 1, will return a list with 2 elements!
x = txt.split('#', 1)
print(x)

endsnippet

snippet .splitlines "splitlines"
.splitlines()

endsnippet

snippet string.splitlines=> "string.splitlines=>"
txt = 'Thank you for the music
Welcome to the jungle'
x = txt.splitlines(True)
print(x)

endsnippet

snippet .startswith "startswith"
.startswith()

endsnippet

snippet string.startswith=> "string.startswith=>"
txt = 'Hello, welcome to my world.'
x = txt.startswith('wel', 7, 20)
print(x)

endsnippet

snippet .swapcase "swapcase"
.swapcase()

endsnippet

snippet string.swapcase=> "string.swapcase=>"
txt = 'Hello My Name Is PETER'
x = txt.swapcase()
print(x)

endsnippet

snippet .title "title"
.title()

endsnippet

snippet string.title=> "string.title=>"
txt = 'hello b2b2b2 and 3g3g3g'
x = txt.title()
print(x)

endsnippet

snippet .translate "translate"
.translate()

endsnippet

snippet string.translate=> "string.translate=>"
# translation table - a dictionary
translation = {97: None, 98: None, 99: 105}

string = 'abcdef'
print('Original string:', string)

# translate string
print('Translated string:', string.translate(translation))

endsnippet

snippet .upper "upper"
.upper()

endsnippet

snippet string.upper=> "string.upper=>"
txt = 'Hello my friends'
x = txt.upper()
print(x)

endsnippet

snippet .zfill "zfill"
.zfill()

endsnippet

snippet string.zfill=> "string.zfill=>"
txt = '50'
x = txt.zfill(10)
print(x)

endsnippet

snippet .append "append"
.append()

endsnippet

snippet list.append=> "list.append=>"
a = ['apple', 'banana', 'cherry']
b = ['Ford', 'BMW', 'Volvo']
a.append(b)
print(a)

endsnippet

snippet .clear "clear"
.clear()

endsnippet

snippet list.clear=> "list.clear=>"
fruits = ['apple', 'banana', 'cherry']
fruits.clear()
print(fruits)

endsnippet

snippet .copy "copy"
.copy()

endsnippet

snippet list.copy=> "list.copy=>"
fruits = ['apple', 'banana', 'cherry']
x = fruits.copy()
print(x)

endsnippet

snippet .count "list.count"
.count

endsnippet

snippet list.count=> "list.count=>"
fruits = [1, 4, 2, 9, 7, 8, 9, 3, 1]
x = fruits.count(9)
print(x)

endsnippet

snippet .extend "extend"
.extend()

endsnippet

snippet list.extend=> "list.extend=>"
fruits = ['apple', 'banana', 'cherry']
points = (1, 4, 5, 9)
fruits.extend(points)
print(fruits)

endsnippet

snippet list.index=> "list.index=>"
fruits = [4, 55, 64, 32, 16, 32]
x = fruits.index(32)
print(x)

endsnippet

snippet .insert "insert"
.insert()

endsnippet

snippet list.insert=> "list.insert=>"
fruits = ['apple', 'banana', 'cherry']
x = fruits.insert(1, 'orange')
print(x)

endsnippet

snippet .pop "pop"
.pop()

endsnippet

snippet list.pop=> "list.pop=>"
fruits = ['apple', 'banana', 'cherry']
fruits.pop(1)
print(fruits)

endsnippet

snippet .remove "remove"
.remove()

endsnippet

snippet list.remove=> "list.remove=>"
fruits = ['apple', 'banana', 'cherry']
fruits.remove('banana')
print(fruits)

endsnippet

snippet .reverse "reverse"
.reverse()

endsnippet

snippet list.reverse=> "list.reverse=>"
fruits = ['apple', 'banana', 'cherry']
fruits.reverse()
print(fruits)

endsnippet

snippet .sort "sort"
.sort()

endsnippet

snippet list.sort=> "list.sort=>"
# A function that returns the length of the value:
def myFunc(e):
  return len(e)
cars = ['Ford', 'Mitsubishi', 'BMW', 'VW']
cars.sort(reverse=True, key=myFunc)
print(cars)

endsnippet

snippet comp=> "comprehensions"
[ expression for item in list if conditional ]

endsnippet

snippet list.comp=>_1 "list.comp=>_1"
x = [i for i in range(10)]
print(x)

endsnippet

snippet list.comp=>_2 "list.comp=>_2"
x = [x**2 for x in range(10)]
print(x)

endsnippet

snippet list.comp=>_3 "list.comp=>_3"
list1 = [3,4,5]
multiplied = [item*3 for item in list1]
print(multiplied)

endsnippet

snippet list.comp=>_4 "list.comp=>_4"
listOfWords = ['this','is','a','list','of','words']
items = [ word[0] for word in listOfWords ]
print(items)

endsnippet

snippet list.comp=>_5 "list.comp=>_5"
x = [double(x) for x in range(10) if x%2==0]
print(x)

endsnippet

snippet dictionary.clear=> "dictionary.clear=>"
car = {
  'brand': 'Ford',
  'model': 'Mustang',
  'year': 1964
}
car.clear()
print(car)

endsnippet

snippet dictionary.copy=> "dictionary.copy=>"
car = {
  'brand': 'Ford',
  'model': 'Mustang',
  'year': 1964
}
x = car.copy()
print(x)

endsnippet

snippet .fromkeys "fromkeys"
.fromkeys(x, y)

endsnippet

snippet dictionary.fromkeys=> "dictionary.fromkeys=>"
x = ('key1', 'key2', 'key3')
thisdict = dict.fromkeys(x)
print(thisdict)

endsnippet

snippet .get "get"
.get()

endsnippet

snippet dictionary.get=> "dictionary.get=>"
car = {
  'brand': 'Ford',
  'model': 'Mustang',
  'year': 1964
}
x = car.get('price', 15000)
print(x)

endsnippet

snippet .items "items"
.items()

endsnippet

snippet dictionary.items=> "dictionary.items=>"
car = {
  'brand': 'Ford',
  'model': 'Mustang',
  'year': 1964
}
x = car.items()
car['year'] = 2018
print(x)

endsnippet

snippet .keys "keys"
.keys()

endsnippet

snippet dictionary.keys=> "dictionary.keys=>"
car = {
  'brand': 'Ford',
  'model': 'Mustang',
  'year': 1964
}
x = car.keys()
car['color'] = 'white'
print(x)

endsnippet

snippet dictionary.pop=> "dictionary.pop=>"
car = {
  'brand': 'Ford',
  'model': 'Mustang',
  'year': 1964
}
car.pop('model')
print(car)

endsnippet

snippet .popitem "popitem"
.popitem()

endsnippet

snippet dictionary.popitem=> "dictionary.popitem=>"
car = {
  'brand': 'Ford',
  'model': 'Mustang',
  'year': 1964
}
car.popitem()
print(car)

endsnippet

snippet .setdefault "setdefault"
.setdefault()

endsnippet

snippet dictionary.setdefault=> "dictionary.setdefault=>"
car = {
  'brand': 'Ford',
  'model': 'Mustang',
  'year': 1964
}
x = car.setdefault('color', 'white')
print(x)

endsnippet

snippet .update "update"
x.update(y)

endsnippet

snippet dictionary.update=> "dictionary.update=>"
car = {
  'brand': 'Ford',
  'model': 'Mustang',
  'year': 1964
}
car.update({'color': 'White'})
print(car)

endsnippet

snippet .values "values"
.values()

endsnippet

snippet dictionary.values=> "dictionary.values=>"
car = {
  'brand': 'Ford',
  'model': 'Mustang',
  'year': 1964
}
x = car.values()
car['year'] = 2018
print(x)

endsnippet

snippet .count "tuple.count"
.count(value)

endsnippet

snippet tuple.count=> "tuple.count=>"
thistuple = (1, 3, 7, 8, 7, 5, 4, 6, 8, 5)
x = thistuple.count(5)
print(x)

endsnippet

snippet tuple.index=> "tuple.index=>"
thistuple = (1, 3, 7, 8, 7, 5, 4, 6, 8, 5)
x = thistuple.index(8)
print(x)

endsnippet

snippet .add "add"
.add()

endsnippet

snippet sets.add=> "sets.add=>"
fruits = {'apple', 'banana', 'cherry'}
fruits.add('orange')
print(fruits)

endsnippet

snippet sets.clear=> "sets.clear=>"
fruits = {'apple', 'banana', 'cherry'}
fruits.clear()
print(fruits)

endsnippet

snippet sets.copy=> "sets.copy=>"
fruits = {'apple', 'banana', 'cherry'}
x = fruits.copy()
print(x)

endsnippet

snippet .difference "difference"
x.difference(y)

endsnippet

snippet sets.difference=>_1 "sets.difference=>_1"
x = {'apple', 'banana', 'cherry'}
y = {'google', 'microsoft', 'apple'}
z = x.difference(y)
print(z)

endsnippet

snippet sets.difference=>_2 "sets.difference=>_2"
x = {'apple', 'banana', 'cherry'}
y = {'google', 'microsoft', 'apple'}
z = y.difference(x)
print(z)

endsnippet

snippet .difference_update "difference_update"
x.difference_update(y)

endsnippet

snippet sets.difference_update=> "sets.difference_update=>"
x = {'apple', 'banana', 'cherry'}
y = {'google', 'microsoft', 'apple'}
x.difference_update(y)
print(x)

endsnippet

snippet .discard "discard"
.discard()

endsnippet

snippet sets.discard=> "sets.discard=>"
fruits = {'apple', 'banana', 'cherry'}
fruits.discard('banana')
print(fruits)

endsnippet

snippet .intersection "intersection"
x.intersection(y)

endsnippet

snippet sets.intersection=>_1 "sets.intersection=>_1"
x = {'apple', 'banana', 'cherry'}
y = {'google', 'microsoft', 'apple'}
z = x.intersection(y)
print(z)

endsnippet

snippet sets.intersection=>_2 "sets.intersection=>_2"
x = {'a', 'b', 'c'}
y = {'c', 'd', 'e'}
z = {'f', 'g', 'c'}
result = x.intersection(y, z)
print(result)

endsnippet

snippet .intersection_update "intersection_update"
x.intersection_update(y)

endsnippet

snippet sets.intersection_update=>_1 "sets.intersection_update=>_1"
x = {'apple', 'banana', 'cherry'}
y = {'google', 'microsoft', 'apple'}
x.intersection_update(y)
print(x)

endsnippet

snippet sets.intersection_update=>_2 "sets.intersection_update=>_2"
x = {'a', 'b', 'c'}
y = {'c', 'd', 'e'}
z = {'f', 'g', 'c'}
x.intersection_update(y, z)
print(x)

endsnippet

snippet .isdisjoint "isdisjoint"
x.isdisjoint(y)

endsnippet

snippet sets.isdisjoint=>_1 "sets.isdisjoint=>_1"
x = {'apple', 'banana', 'cherry'}
y = {'google', 'microsoft', 'facebook'}
z =
print(z)

endsnippet

snippet sets.isdisjoint=>_2 "sets.isdisjoint=>_2"
x = {'apple', 'banana', 'cherry'}
y = {'google', 'microsoft', 'apple'}
z = x.isdisjoint(y)
print(z)

endsnippet

snippet .issubset "issubset"
x.issubset(y)

endsnippet

snippet sets.sets.issubset=>_1 "sets.issubset=>_1"
x = {'a', 'b', 'c'}
y = {'f', 'e', 'd', 'c', 'b', 'a'}
z = x.issubset(y)
print(z)

endsnippet

snippet sets.issubset=>_2 "sets.issubset=>_2"
x = {'a', 'b', 'c'}
y = {'f', 'e', 'd', 'c', 'b'}
z = x.issubset(y)
print(z)

endsnippet

snippet .issuperset "issuperset"
x.issuperset(y)

endsnippet

snippet sets.issuperset=>_1 "sets.issuperset=>_1"
x = {'f', 'e', 'd', 'c', 'b', 'a'}
y = {'a', 'b', 'c'}
z = x.issuperset(y)
print(z)

endsnippet

snippet sets.issuperset=>_2 "sets.issuperset=>_2"
x = {'f', 'e', 'd', 'c', 'b'}
y = {'a', 'b', 'c'}
z = x.issuperset(y)
print(z)

endsnippet

snippet sets.pop=> "sets.pop=>"
fruits = {'apple', 'banana', 'cherry'}
fruits.pop()
print(fruits)

endsnippet

snippet sets.remove=> "sets.remove=>"
fruits = {'apple', 'banana', 'cherry'}
fruits.remove('banana')
print(fruits)

endsnippet

snippet .symmetric_difference "symmetric_difference"
x.symmetric_difference(y)

endsnippet

snippet sets.symmetric_difference=> "sets.symmetric_difference=>"
x = {'apple', 'banana', 'cherry'}
y = {'google', 'microsoft', 'apple'}
z = x.symmetric_difference(y)
print(z)

endsnippet

snippet .symmetric_difference_update "symmetric_difference_update"
x.symmetric_difference_update(y)

endsnippet

snippet sets.symmetric_difference_update=> "sets.symmetric_difference_update=>"
x = {'apple', 'banana', 'cherry'}
y = {'google', 'microsoft', 'apple'}
x.symmetric_difference_update(y)
print(x)

endsnippet

snippet .union "union"
x.union(y)

endsnippet

snippet sets.union=>_1 "sets.union=>_1"
x = {'apple', 'banana', 'cherry'}
y = {'google', 'microsoft', 'apple'}
z = x.union(y)
print(z)

endsnippet

snippet sets.union=>_2 "sets.union=>_2"
x = {'a', 'b', 'c'}
y = {'f', 'd', 'a'}
z = {'c', 'd', 'e'}
result = x.union(y, z)
print(result)

endsnippet

snippet sets.update=> "sets.update=>"
x = {'apple', 'banana', 'cherry'}
y = {'google', 'microsoft', 'apple'}
x.update(y)
print(x)

endsnippet

snippet class=> "class"
class MyClass:
  pass

endsnippet

snippet __init__=> "__init__"
def __init__(self, name, age):
  self.name = name
  self.age = age

endsnippet

snippet __iter__=> "__iter__"
def __iter__(self):
  self.a = 1
  return self

endsnippet

snippet __next__=> "__next__"
def __next__(self):
  x = self.a
  self.a += 1
  return x

endsnippet

snippet import=> "import"
import mymodule as mx

endsnippet

snippet trye=> "tryexcept"
try:
  print(x)
except:
  print('An exception occurred')

endsnippet

snippet tryef=> "tryexceptfinally"
try:
  print(x)
except:
  print('Something went wrong')
finally:
  print('The try except is finished')

endsnippet

snippet file=>openFile "openFile"
f = open('demofile.txt', 'r')
print(f.read())

endsnippet

snippet file=>openFileReadLine "openFileReadLine"
f = open('demofile.txt', 'r')
print(f.readline())

endsnippet

snippet file=>writeExistFile "writeExistFile"
f = open('demofile.txt', 'a')
f.write('Now the file has one more line!')

endsnippet

snippet file=>writeOwerWrite "writeOwerWrite"
f = open('demofile.txt', 'w')
f.write('Woops! I have deleted the content!')

endsnippet

snippet file=>createFileIfDoesNotExist "createFileIfDoesNotExist"
f = open('myfile.txt', 'w')

endsnippet

snippet file=>createFile "createFile"
f = open('myfile.txt', 'x')

endsnippet

snippet file=>deleteFile "deleteFile"
#import os
os.remove('demofile.txt')

endsnippet

snippet class=>_1 "class=>_1"
class Person:
    pass  # An empty block
p = Person()
print(p)

endsnippet

snippet class=>inheritance_1 "class=>inheritance_1"
class Bird:

   def __init__(self):
     print('Bird is ready')

   def whoisThis(self):
     print('Bird')

   def swim(self):
     print('Swim faster')

# child class
class Penguin(Bird):

   def __init__(self):
     # call super() function
     super().__init__()
     print('Penguin is ready')

   def whoisThis(self):
     print('Penguin')

   def run(self):
     print('Run faster')

peggy = Penguin()
peggy.whoisThis()
peggy.swim()
peggy.run()

endsnippet

snippet class=>inheritance_2 "class=>inheritance_2"
class SchoolMember:
    '''Represents any school member.'''
    def __init__(self, name, age):
        self.name = name
        self.age = age
        print('(Initialized SchoolMember: {})'.format(self.name))
    def tell(self):
        '''Tell my details.'''
        print('Name:{} Age:{}'.format(self.name, self.age), end=' ')
class Teacher(SchoolMember):
    '''Represents a teacher.'''
    def __init__(self, name, age, salary):
        SchoolMember.__init__(self, name, age)
        self.salary = salary
        print('(Initialized Teacher: {})'.format(self.name))
    def tell(self):
        SchoolMember.tell(self)
        print('Salary: {:d}'.format(self.salary))
class Student(SchoolMember):
    '''Represents a student.'''
    def __init__(self, name, age, marks):
        SchoolMember.__init__(self, name, age)
        self.marks = marks
        print('(Initialized Student: {})'.format(self.name))
    def tell(self):
        SchoolMember.tell(self)
        print('Marks: {:d}'.format(self.marks))
t = Teacher('Mrs. Shrividya', 40, 30000)
s = Student('Swaroop', 25, 75)
# prints a blank line
print()
members = [t, s]
for member in members:
    # Works for both Teachers and Students
    member.tell()

endsnippet

snippet class=>with_attribute_1 "class=>with_attribute_1"
class Parrot:

# class attribute
 species = 'bird'

# instance attribute
 def __init__(self, name, age):
    self.name = name
    self.age = age

# instantiate the Parrot class
blu = Parrot('Blu', 10)
woo = Parrot('woo', 15)

# access the class attributes
print('Blu is a {}'.format(blu.__class__.species))
print('Woo is also a {}'.format(woo.__class__.species))
# access the instance attributes
print('{} is {} years old'.format( blu.name, blu.age))
print('{} is {} years old'.format( woo.name, woo.age))

endsnippet

snippet class=>with_attribute_2 "class=>with_attribute_2"
class Person:
    def __init__(self, name):
        self.name = name
    def say_hi(self):
        print('Hello, my name is', self.name)
p = Person('Swaroop')
p.say_hi()
# The previous 2 lines can also be written as
# Person('Swaroop').say_hi()

endsnippet

snippet class=>with_attribute_3 "class=>with_attribute_3"
class Robot:
    '''Represents a robot, with a name.'''
    # A class variable, counting the number of robots
    population = 0
    def __init__(self, name):
        '''Initializes the data.'''
        self.name = name
        print('(Initializing {})'.format(self.name))
        # When this person is created, the robot
        # adds to the population
        Robot.population += 1
    def die(self):
        '''I am dying.'''
        print('{} is being destroyed!'.format(self.name))
        Robot.population -= 1
        if Robot.population == 0:
            print('{} was the last one.'.format(self.name))
        else:
            print('There are still {:d} robots working.'.format(
                Robot.population))
    def say_hi(self):
        '''Greeting by the robot.
        Yeah, they can do that.'''
        print('Greetings, my masters call me {}.'.format(self.name))
    @classmethod
    def how_many(cls):
        '''Prints the current population.'''
        print('We have {:d} robots.'.format(cls.population))
droid1 = Robot('R2-D2')
droid1.say_hi()
Robot.how_many()
droid2 = Robot('C-3PO')
droid2.say_hi()
Robot.how_many()
print('Robots can do some work here.')
print('Robots have finished their work. So lets destroy them.')
droid1.die()
droid2.die()
Robot.how_many()

endsnippet

snippet class=>with_method_1 "class=>with_method_1"
class Parrot:

# instance attributes
 def __init__(self, name, age):
   self.name = name
   self.age = age

# instance method
 def sing(self, song):
   return '{} sings {}'.format(self.name, song)

 def dance(self):
   return '{} is now dancing'.format(self.name)

# instantiate the object
blu = Parrot('Blu', 10)
# call our instance methods
print(blu.sing('Happy'))
print(blu.dance())

endsnippet

snippet class=>with_method_2 "class=>with_method_2"
class Person:
    def say_hi(self):
        print('Hello, how are you?')
p = Person()
p.say_hi()
# The previous 2 lines can also be written as
# Person().say_hi()

endsnippet

snippet class=>encapsulation "class=>encapsulation"
class Computer:

 def __init__(self):
   self.__maxprice = 900

 def sell(self):
   print('Selling Price: {}'.format(self.__maxprice))

 def setMaxPrice(self, price):
   self.__maxprice = price

c = Computer()
c.sell()

# change the price
c.__maxprice = 1000
c.sell()

# using setter function
c.setMaxPrice(1000)
c.sell()

endsnippet

snippet class=>polymorphism "class=>polymorphism"
class Parrot:

 def fly(self):
   print('Parrot can fly')

 def swim(self):
   print('Parrot can not swim')

class Penguin:

 def fly(self):
   print('Penguin can not fly')

 def swim(self):
   print('Penguin can swim')

# common interface
def flying_test(bird):
  bird.fly()

#instantiate objects
blu = Parrot()
peggy = Penguin()

# passing the object
flying_test(blu)
flying_test(peggy)

endsnippet

snippet tfk:import "Import Tensorflow Keras"
import tensorflow as tf
from tensorflow import keras

endsnippet

snippet tfk:dataset:mnist "Load MNIST"
(${1:x_train}, ${2:y_train}), (${3:x_test}, ${4:y_test}) = keras.datasets.mnist.load_data()

endsnippet

snippet tfk:dataset:cifar10 "Load Cifar10"
(${1:x_train}, ${2:y_train}), (${3:x_test}, ${4:y_test}) = keras.datasets.cifar10.load_data()

endsnippet

snippet tfk:dataset:cifar100 "Load Cifar100"
(${1:x_train}, ${2:y_train}), (${3:x_test}, ${4:y_test}) = keras.datasets.cifar100.load_data()

endsnippet

snippet tfk:dataset:celeb_a "Load celeb_a"
# import tensorflow_datasets as tfds
(train_dataset, test_dataset, vali_datset), info = tfds.load('celeb_a',
                                                             split=['train',
                                                                    'test',
                                                                    'validation'],
                                                             with_info=True)

endsnippet

snippet tfk:dataset:coco "Load coco"
# import tensorflow_datasets as tfds
dataset, info = tfds.load('coco',with_info=True)

endsnippet

snippet tfk:dataset:imagenet2012 "Load imagenet2012"
# import tensorflow_datasets as tfds
#! Warning: Manually default download to ~/tensorflow_datasets/manual/imagenet2012
(train_dataset, test_dataset), info = tfds.load('imagenet2012',
                          split=["train", "validation"],
                          with_info=True)

endsnippet

snippet tfk:dataset:voc2007 "Load voc"
# import tensorflow_datasets as tfds
dataset, info = tfds.load('voc/2007',with_info=True)

endsnippet

snippet tfk:dataset:iris "Load iris"
# import tensorflow_datasets as tfds
dataset, info = tfds.load('iris',with_info=True)

endsnippet

snippet tfk:dataset:cycle_gan "Load cycle_gan"
# import tensorflow_datasets as tfds
dataset, info = tfds.load('cycle_gan',with_info=True)

endsnippet

snippet tfk:dataset:scientific_papers "Load scientific_papers"
# import tensorflow_datasets as tfds
dataset, info = tfds.load('scientific_papers',with_info=True)

endsnippet

snippet tfk:dataset:scicite "Load scicite"
# import tensorflow_datasets as tfds
dataset, info = tfds.load('scicite',with_info=True)

endsnippet

snippet tfk:dataset:flores "Load flores"
# import tensorflow_datasets as tfds
dataset, info = tfds.load('flores',with_info=True)

endsnippet

snippet tfk:dataset:moving_mnist "Load moving_mnist"
# import tensorflow_datasets as tfds
dataset, info = tfds.load('moving_mnist',with_info=True)

endsnippet

snippet tfk:dataset:fmnist "Load Fashion MNIST"
(${1:x_train}, ${2:y_train}), (${3:x_test}, ${4:y_test}) = keras.datasets.fashion_mnist.load_data()

endsnippet

snippet tfk:ctrl:model "Model Block"
class ${1:MyModel}(tf.keras.Model):

  def __init__(self):
    super(${1:MyModel}, self).__init__()
    self.dense1 = tf.keras.layers.${2:[Dense Like Layer]}

  def call(self, inputs):
    x = self.dense1(inputs)
    return x

endsnippet

snippet tfk:ctrl:fit "Fit Op"
$1.fit(x=$2, y=$3,
          batch_size=None, epochs=$4,
          verbose=1, validation_data=None,
          steps_per_epoch=None, validation_steps=None,
          validation_batch_size=None, validation_freq=1)

endsnippet

snippet tfk:ctrl:evaluate "Evaluate Op"
$1.evaluate($2, $3, batch_size=None, verbose=1,
               sample_weight=None, steps=None, callbacks=None, max_queue_size=10)

endsnippet

snippet tfk:ctrl:compile "Compile Op"
$1.compile(optimizer='${2:adam}', loss=None, metrics=['accuracy'], loss_weights=None,
              sample_weight_mode=None, weighted_metrics=None)

endsnippet

snippet tfk:ctrl:callback "Callback Class"
class ${1:FitCallback}(tf.keras.callbacks.Callback):
    def on_epoch_${2:end}(self, epoch, logs=None):
        pass

endsnippet

snippet tfk:ctrl:saveModel "Save Model Op"
tf.keras.models.save_model(
    ${1:model}, ${2:filePathString}, overwrite=True,
    include_optimizer=True, save_format=None,
    signatures=None, options=None)

endsnippet

snippet tfk:ctrl:loadModel "Load Model Op"
tf.keras.models.load_model(
    ${1:filePathString},
    custom_objects=None, compile=True)

endsnippet

snippet tfk:code:mnist:simple "Simple Code Frame of Mnist"
from tensorflow import keras

mnist = keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(${1:128}, activation='${2:relu}'),
    keras.layers.Dropout(${3: 0.2}),
    keras.layers.Dense(${4:10}, activation='${5:softmax}')
])

model.compile(optimizer='${6:adam}',
              loss='${7:sparse_categorical_crossentropy}',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=${8:5})

model.evaluate(x_test,  y_test, verbose=2)

endsnippet

snippet tfk:code:mnist:full "Code Frame of Mnist"
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.keras import Model

mnist = keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

x_train = x_train[..., tf.newaxis]
x_test = x_test[..., tf.newaxis]

train_ds = tf.data.Dataset.from_tensor_slices(
    (x_train, y_train)).shuffle(${1:10000}).batch(${2:32})
test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(${2:32})

class MyModel(Model):
    def __init__(self):
        super(MyModel, self).__init__()
        self.conv1 = Conv2D(32, 3, activation='relu')
        self.flatten = Flatten()
        self.d1 = Dense(128, activation='relu')
        self.d2 = Dense(10, activation='softmax')

    def call(self, x):
        x = self.conv1(x)
        x = self.flatten(x)
        x = self.d1(x)
        return self.d2(x)


model = MyModel()

loss_object = tf.keras.losses.SparseCategoricalCrossentropy()

optimizer = tf.keras.optimizers.Adam()

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
    name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
    name='test_accuracy')


@tf.function
def train_step(images, labels):
    with tf.GradientTape() as tape:
        predictions = model(images)
        loss = loss_object(labels, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    train_loss(loss)
    train_accuracy(labels, predictions)


@tf.function
def test_step(images, labels):
    predictions = model(images)
    t_loss = loss_object(labels, predictions)

    test_loss(t_loss)
    test_accuracy(labels, predictions)


EPOCHS = 5

for epoch in range(EPOCHS):
    train_loss.reset_states()
    train_accuracy.reset_states()
    test_loss.reset_states()
    test_accuracy.reset_states()

    for images, labels in train_ds:
        train_step(images, labels)

    for test_images, test_labels in test_ds:
        test_step(test_images, test_labels)

    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
    print(template.format(epoch+1,
                          train_loss.result(),
                          train_accuracy.result()*100,
                          test_loss.result(),
                          test_accuracy.result()*100))

endsnippet

snippet tfk:code:oxford "Model Framework of Oxford"
# run command: pip install -q git+https://github.com/tensorflow/examples.git
# run command: pip install tensorflow_datasets
import matplotlib.pyplot as plt
from IPython.display import clear_output
import tensorflow as tf

from tensorflow_examples.models.pix2pix import pix2pix

import tensorflow_datasets as tfds
tfds.disable_progress_bar()

dataset, info = tfds.load('oxford_iiit_pet:3.1.0', with_info=True)


def normalize(input_image, input_mask):
    input_image = tf.cast(input_image, tf.float32)/255.0
    input_mask -= 1
    return input_image, input_mask


@tf.function
def load_image_train(datapoint):
    input_image = tf.image.resize(datapoint['image'], (128, 128))
    input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))

    if tf.random.uniform(()) > 0.5:
        input_image = tf.image.flip_left_right(input_image)
        input_mask = tf.image.flip_left_right(input_mask)

    input_image, input_mask = normalize(input_image, input_mask)

    return input_image, input_mask


def load_image_test(datapoint):
    input_image = tf.image.resize(datapoint['image'], (128, 128))
    input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))

    input_image, input_mask = normalize(input_image, input_mask)

    return input_image, input_mask


TRAIN_LENGTH = info.splits['train'].num_examples
BATCH_SIZE = 64
BUFFER_SIZE = 1000
STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE

train = dataset['train'].map(
    load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)
test = dataset['test'].map(load_image_test)

train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()
train_dataset = train_dataset.prefetch(
    buffer_size=tf.data.experimental.AUTOTUNE)
test_dataset = test.batch(BATCH_SIZE)


def display(display_list):
    plt.figure(figsize=(15, 15))

    title = ['Input Image', 'True Mask', 'Predicted Mask']

    for i in range(len(display_list)):
        plt.subplot(1, len(display_list), i+1)
        plt.title(title[i])
        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))
        plt.axis('off')
    plt.show()


for image, mask in train.take(1):
    sample_image, sample_mask = image, mask
display([sample_image, sample_mask])

OUTPUT_CHANNELS = 3

base_model = tf.keras.applications.MobileNetV2(
    input_shape=[128, 128, 3], include_top=False)

layer_names = [
    'block_1_expand_relu',   # 64x64
    'block_3_expand_relu',   # 32x32
    'block_6_expand_relu',   # 16x16
    'block_13_expand_relu',  # 8x8
    'block_16_project',      # 4x4
]
layers = [base_model.get_layer(name).output for name in layer_names]

down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)

down_stack.trainable = False

up_stack = [
    pix2pix.upsample(512, 3),  # 4x4 -> 8x8
    pix2pix.upsample(256, 3),  # 8x8 -> 16x16
    pix2pix.upsample(128, 3),  # 16x16 -> 32x32
    pix2pix.upsample(64, 3),   # 32x32 -> 64x64
]


def unet_model(output_channels):

    last = tf.keras.layers.Conv2DTranspose(
        output_channels, 3, strides=2,
        padding='same', activation='softmax')  # 64x64 -> 128x128

    inputs = tf.keras.layers.Input(shape=[128, 128, 3])
    x = inputs

    skips = down_stack(x)
    x = skips[-1]
    skips = reversed(skips[:-1])

    for up, skip in zip(up_stack, skips):
        x = up(x)
        concat = tf.keras.layers.Concatenate()
        x = concat([x, skip])

    x = last(x)

    return tf.keras.Model(inputs=inputs, outputs=x)

model = unet_model(OUTPUT_CHANNELS)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

def create_mask(pred_mask):
  pred_mask = tf.argmax(pred_mask, axis=-1)
  pred_mask = pred_mask[..., tf.newaxis]
  return pred_mask[0]

def show_predictions(dataset=None, num=1):
  if dataset:
    for image, mask in dataset.take(num):
      pred_mask = model.predict(image)
      display([image[0], mask[0], create_mask(pred_mask)])
  else:
    display([sample_image, sample_mask,
             create_mask(model.predict(sample_image[tf.newaxis, ...]))])

show_predictions()

class DisplayCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs=None):
    clear_output(wait=True)
    show_predictions()
    print ('
Sample Prediction after epoch {}
'.format(epoch+1))

EPOCHS = 20
VAL_SUBSPLITS = 5
VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS

model_history = model.fit(train_dataset, epochs=EPOCHS,
                          steps_per_epoch=STEPS_PER_EPOCH,
                          validation_steps=VALIDATION_STEPS,
                          validation_data=test_dataset,
                          callbacks=[DisplayCallback()])

loss = model_history.history['loss']
val_loss = model_history.history['val_loss']

epochs = range(EPOCHS)

plt.figure()
plt.plot(epochs, loss, 'r', label='Training loss')
plt.plot(epochs, val_loss, 'bo', label='Validation loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss Value')
plt.ylim([0, 1])
plt.legend()
plt.show()

show_predictions(test_dataset, 3)

endsnippet

snippet tfk:code:translate "Translate"
import tensorflow as tf

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from sklearn.model_selection import train_test_split

import unicodedata
import re
import numpy as np
import os
import io
import time

path_to_zip = tf.keras.utils.get_file(
    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',
    extract=True)

path_to_file = os.path.dirname(path_to_zip)+"/spa-eng/spa.txt"

# Converts the unicode file to ascii


def unicode_to_ascii(s):
    return ''.join(c for c in unicodedata.normalize('NFD', s)
                   if unicodedata.category(c) != 'Mn')


def preprocess_sentence(w):
    w = unicode_to_ascii(w.lower().strip())

    # creating a space between a word and the punctuation following it
    # eg: "he is a boy." => "he is a boy ."
    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation
    w = re.sub(r"([?.!,Â¿])", r" \1 ", w)
    w = re.sub(r'[" "]+', " ", w)

    # replacing everything with space except (a-z, A-Z, ".", "?", "!", ",")
    w = re.sub(r"[^a-zA-Z?.!,Â¿]+", " ", w)

    w = w.rstrip().strip()

    # adding a start and an end token to the sentence
    # so that the model know when to start and stop predicting.
    w = '<start> ' + w + ' <end>'
    return w


en_sentence = u"May I borrow this book?"
sp_sentence = u"Â¿Puedo tomar prestado este libro?"
print(preprocess_sentence(en_sentence))
print(preprocess_sentence(sp_sentence).encode('utf-8'))

# 1. Remove the accents
# 2. Clean the sentences
# 3. Return word pairs in the format: [ENGLISH, SPANISH]


def create_dataset(path, num_examples):
    lines = io.open(path, encoding='UTF-8').read().strip().split('\n')

    word_pairs = [[preprocess_sentence(w) for w in l.split(
        '\t')] for l in lines[:num_examples]]

    return zip(*word_pairs)


en, sp = create_dataset(path_to_file, None)
print(en[-1])
print(sp[-1])


def max_length(tensor):
    return max(len(t) for t in tensor)


def tokenize(lang):
    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(
        filters='')
    lang_tokenizer.fit_on_texts(lang)

    tensor = lang_tokenizer.texts_to_sequences(lang)

    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,
                                                           padding='post')

    return tensor, lang_tokenizer


def load_dataset(path, num_examples=None):
    # creating cleaned input, output pairs
    targ_lang, inp_lang = create_dataset(path, num_examples)

    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)
    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)

    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer


# Try experimenting with the size of that dataset
num_examples = 30000
input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(
    path_to_file, num_examples)

# Calculate max_length of the target tensors
max_length_targ, max_length_inp = max_length(
    target_tensor), max_length(input_tensor)

# Creating training and validation sets using an 80-20 split
input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(
    input_tensor, target_tensor, test_size=0.2)


BUFFER_SIZE = len(input_tensor_train)
BATCH_SIZE = 64
steps_per_epoch = len(input_tensor_train)//BATCH_SIZE
embedding_dim = 256
units = 1024
vocab_inp_size = len(inp_lang.word_index)+1
vocab_tar_size = len(targ_lang.word_index)+1

dataset = tf.data.Dataset.from_tensor_slices(
    (input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)
dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)

example_input_batch, example_target_batch = next(iter(dataset))
example_input_batch.shape, example_target_batch.shape


class Encoder(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):
        super(Encoder, self).__init__()
        self.batch_sz = batch_sz
        self.enc_units = enc_units
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.gru = tf.keras.layers.GRU(self.enc_units,
                                       return_sequences=True,
                                       return_state=True,
                                       recurrent_initializer='glorot_uniform')

    def call(self, x, hidden):
        x = self.embedding(x)
        output, state = self.gru(x, initial_state=hidden)
        return output, state

    def initialize_hidden_state(self):
        return tf.zeros((self.batch_sz, self.enc_units))


encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)

# sample input
sample_hidden = encoder.initialize_hidden_state()
sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)
print('Encoder output shape: (batch size, sequence length, units) {}'.format(
    sample_output.shape))
print('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))


class BahdanauAttention(tf.keras.layers.Layer):
    def __init__(self, units):
        super(BahdanauAttention, self).__init__()
        self.W1 = tf.keras.layers.Dense(units)
        self.W2 = tf.keras.layers.Dense(units)
        self.V = tf.keras.layers.Dense(1)

    def call(self, query, values):
        # query hidden state shape == (batch_size, hidden size)
        # query_with_time_axis shape == (batch_size, 1, hidden size)
        # values shape == (batch_size, max_len, hidden size)
        # we are doing this to broadcast addition along the time axis to calculate the score
        query_with_time_axis = tf.expand_dims(query, 1)

        # score shape == (batch_size, max_length, 1)
        # we get 1 at the last axis because we are applying score to self.V
        # the shape of the tensor before applying self.V is (batch_size, max_length, units)
        score = self.V(tf.nn.tanh(
            self.W1(query_with_time_axis) + self.W2(values)))

        # attention_weights shape == (batch_size, max_length, 1)
        attention_weights = tf.nn.softmax(score, axis=1)

        # context_vector shape after sum == (batch_size, hidden_size)
        context_vector = attention_weights * values
        context_vector = tf.reduce_sum(context_vector, axis=1)

        return context_vector, attention_weights


attention_layer = BahdanauAttention(10)
attention_result, attention_weights = attention_layer(
    sample_hidden, sample_output)

print("Attention result shape: (batch size, units) {}".format(attention_result.shape))
print("Attention weights shape: (batch_size, sequence_length, 1) {}".format(
    attention_weights.shape))


class Decoder(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):
        super(Decoder, self).__init__()
        self.batch_sz = batch_sz
        self.dec_units = dec_units
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.gru = tf.keras.layers.GRU(self.dec_units,
                                       return_sequences=True,
                                       return_state=True,
                                       recurrent_initializer='glorot_uniform')
        self.fc = tf.keras.layers.Dense(vocab_size)

        # used for attention
        self.attention = BahdanauAttention(self.dec_units)

    def call(self, x, hidden, enc_output):
        # enc_output shape == (batch_size, max_length, hidden_size)
        context_vector, attention_weights = self.attention(hidden, enc_output)

        # x shape after passing through embedding == (batch_size, 1, embedding_dim)
        x = self.embedding(x)

        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)
        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)

        # passing the concatenated vector to the GRU
        output, state = self.gru(x)

        # output shape == (batch_size * 1, hidden_size)
        output = tf.reshape(output, (-1, output.shape[2]))

        # output shape == (batch_size, vocab)
        x = self.fc(output)

        return x, state, attention_weights


decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)

sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),
                                      sample_hidden, sample_output)

print('Decoder output shape: (batch_size, vocab size) {}'.format(
    sample_decoder_output.shape))

optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')


def loss_function(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    loss_ = loss_object(real, pred)

    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask

    return tf.reduce_mean(loss_)


checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder,
                                 decoder=decoder)


@tf.function
def train_step(inp, targ, enc_hidden):
    loss = 0

    with tf.GradientTape() as tape:
        enc_output, enc_hidden = encoder(inp, enc_hidden)

        dec_hidden = enc_hidden

        dec_input = tf.expand_dims(
            [targ_lang.word_index['<start>']] * BATCH_SIZE, 1)

        # Teacher forcing - feeding the target as the next input
        for t in range(1, targ.shape[1]):
            # passing enc_output to the decoder
            predictions, dec_hidden, _ = decoder(
                dec_input, dec_hidden, enc_output)

            loss += loss_function(targ[:, t], predictions)

            # using teacher forcing
            dec_input = tf.expand_dims(targ[:, t], 1)

    batch_loss = (loss / int(targ.shape[1]))

    variables = encoder.trainable_variables + decoder.trainable_variables

    gradients = tape.gradient(loss, variables)

    optimizer.apply_gradients(zip(gradients, variables))

    return batch_loss


EPOCHS = 10

for epoch in range(EPOCHS):
    start = time.time()

    enc_hidden = encoder.initialize_hidden_state()
    total_loss = 0

    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):
        batch_loss = train_step(inp, targ, enc_hidden)
        total_loss += batch_loss

        if batch % 100 == 0:
            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,
                                                         batch,
                                                         batch_loss.numpy()))
    # saving (checkpoint) the model every 2 epochs
    if (epoch + 1) % 2 == 0:
        checkpoint.save(file_prefix=checkpoint_prefix)

    print('Epoch {} Loss {:.4f}'.format(epoch + 1,
                                        total_loss / steps_per_epoch))
    print('Time taken for 1 epoch {} sec\n'.format(time.time() - start))


def evaluate(sentence):
    attention_plot = np.zeros((max_length_targ, max_length_inp))

    sentence = preprocess_sentence(sentence)

    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]
    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],
                                                           maxlen=max_length_inp,
                                                           padding='post')
    inputs = tf.convert_to_tensor(inputs)

    result = ''

    hidden = [tf.zeros((1, units))]
    enc_out, enc_hidden = encoder(inputs, hidden)

    dec_hidden = enc_hidden
    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)

    for t in range(max_length_targ):
        predictions, dec_hidden, attention_weights = decoder(dec_input,
                                                             dec_hidden,
                                                             enc_out)

        # storing the attention weights to plot later on
        attention_weights = tf.reshape(attention_weights, (-1, ))
        attention_plot[t] = attention_weights.numpy()

        predicted_id = tf.argmax(predictions[0]).numpy()

        result += targ_lang.index_word[predicted_id] + ' '

        if targ_lang.index_word[predicted_id] == '<end>':
            return result, sentence, attention_plot

        # the predicted ID is fed back into the model
        dec_input = tf.expand_dims([predicted_id], 0)

    return result, sentence, attention_plot

# function for plotting the attention weights


def plot_attention(attention, sentence, predicted_sentence):
    fig = plt.figure(figsize=(10, 10))
    ax = fig.add_subplot(1, 1, 1)
    ax.matshow(attention, cmap='viridis')

    fontdict = {'fontsize': 14}

    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)
    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)

    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

    plt.show()


def translate(sentence):
    result, sentence, attention_plot = evaluate(sentence)

    print('Input: %s' % (sentence))
    print('Predicted translation: {}'.format(result))

    attention_plot = attention_plot[:len(
        result.split(' ')), :len(sentence.split(' '))]
    plot_attention(attention_plot, sentence.split(' '), result.split(' '))

    # restoring the latest checkpoint in checkpoint_dir
checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))
translate(u'hace mucho frio aqui.')

endsnippet

snippet tfk:code:word_embeddings "Word Embedding"
# Run command: pip install -q tf-nightly

import io
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import tensorflow_datasets as tfds
tfds.disable_progress_bar()

(train_data, test_data), info = tfds.load(
    'imdb_reviews/subwords8k',
    split=(tfds.Split.TRAIN, tfds.Split.TEST),
    with_info=True, as_supervised=True)
encoder = info.features['text'].encoder


train_batches = train_data.shuffle(1000).padded_batch(10)
test_batches = test_data.shuffle(1000).padded_batch(10)

train_batch, train_labels = next(iter(train_batches))

embedding_dim = 16

model = keras.Sequential([
    layers.Embedding(encoder.vocab_size, embedding_dim),
    layers.GlobalAveragePooling1D(),
    layers.Dense(16, activation='relu'),
    layers.Dense(1)
])

model.summary()

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(
    train_batches,
    epochs=10,
    validation_data=test_batches, validation_steps=20)


history_dict = history.history

acc = history_dict['accuracy']
val_acc = history_dict['val_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

plt.figure(figsize=(12, 9))
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.figure(figsize=(12, 9))
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.ylim((0.5, 1))
plt.show()

e = model.layers[0]
weights = e.get_weights()[0]
print(weights.shape)  # shape: (vocab_size, embedding_dim)


encoder = info.features['text'].encoder

out_v = io.open('vecs.tsv', 'w', encoding='utf-8')
out_m = io.open('meta.tsv', 'w', encoding='utf-8')

for num, word in enumerate(encoder.subwords):
    vec = weights[num+1]  # skip 0, it's padding.
    out_m.write(word + "\n")
    out_v.write('\t'.join([str(x) for x in vec]) + "\n")
out_v.close()
out_m.close()

try:
    from google.colab import files
except ImportError:
    pass
else:
    files.download('vecs.tsv')
    files.download('meta.tsv')

endsnippet

snippet DjangoModel "Model"
class ${Name}(models.Model):
    $1

    def __str__(self):
        return $2

    def __unicode__(self):
        return $2


endsnippet

snippet DjangoModelAdmin "Model Admin"
class ${Name}Admin(admin.ModelAdmin):
    list_display = ('$1',)

admin.site.register(${Name}, ${Name}Admin)


endsnippet

snippet blank  "Blank and Null Field Properties"
blank=True, null=True

endsnippet

snippet CharField "CharField"
${name} = models.CharField(max_length=${length}, ${blank=True, null=True})

endsnippet

snippet ForeignKeyField "Foreign Key"
${name} = models.ForeignKey('${TargetModel}', related_name='$1', on_delete=models.CASCADE)

endsnippet

snippet savemethod "Override Save Method"
def save(self, *args, **kwargs):
   $1
   super(${ModelName}, self).save(*args, **kwargs) # Call the real save() method

endsnippet

snippet View "View"
class ${Name}View(View):
    def get(self, request, *args, **kwargs):
        return HttpResponse('GET request!')

    def post(self, request, *args, **kwargs):
        return HttpResponse('POST request!')

endsnippet

snippet ListView "ListView"
class ${ModelName}List(ListView):
    model = ${ModelName}
    context_object_name = '$1'
    template_name='$2'

endsnippet

snippet DetailView "DetailView"
class ${ModelName}Detail(DetailView):
    model = ${ModelName}
    template_name='$2'

endsnippet

snippet get_context_data "get_context_data"
def get_context_data(self, **kwargs):
    context = super(${ViewName}, self).get_context_data(**kwargs)
    return context

endsnippet

snippet __init__ "__init__"
__init__(self, *args, **kwargs):
$1

endsnippet

snippet Traceback "traceback"
import traceback; traceback.print_exc();

endsnippet

snippet pdb "pdb"
import pdb ; pdb.set_trace()

endsnippet

snippet ipdb "ipdb"
import ipdb ; ipdb.set_trace()

endsnippet

snippet pytorch:torchvision:load_dataset_1 "Torchvision Datasets 1"
dataset = torchvision.datasets.${1|MNIST,FashionMNIST,KMNIST,CIFAR10,CIFAR100|}(${2:'path/to/root/'}, train=${3|True,False|}, download=${4|False,True|})

endsnippet

snippet pytorch:torchvision:load_dataset_2 "Torchvision Datasets 2"
dataset = torchvision.datasets.${1|ImageNet,STL10,SVHN|}(root=${2:'path/to/root/'}, split=${3:'train'}, download=${4|False,True|})

endsnippet

snippet pytorch:torchvision:load_dataset_3 "Torchvision Dataset 3"
dataset = torchvision.datasets.${1|CocoCaptions,CocoDetection,Flickr,Flickr30k|}(root=${2:'path/to/root/'}, annFile=${3:'path/to/annotation/file/'})

endsnippet

snippet pytorch:torchvision:image_folder "Torchvision ImageFolder"
dataset = torchvision.datasets.ImageFolder(${1:'path/to/root'})

endsnippet

snippet pytorch:torchvision:dataset_folder "Torchvision DataFolder"
dataset = torchvision.datasets.DataFolder(${1:'path/to/root'}, ${2:loader})

endsnippet

snippet pytorch:torchvision:load_model "Torchvision Model"
model = torchvision.models.${1|alexnet,vgg11,vgg11_bn,vgg13,vgg13_bn,vgg16,vgg16_bn,vgg19,vgg19_bn,resnet18,resnet34,resnet50,resnet101,resnet152,squeezenet1_0,squeezenet1_1,densenet121,densenet169,densenet161,densenet201,inception_v3,googlenet,shufflenet_v2_x0_5,shufflenet_v2_x1_0,shufflenet_v2_x1_5,shufflenet_v2_x2_0,mobilenet_v2,resnext50_32x4d,esnext101_32x8d|}(pretrained=${2|False,True|}, progress=${3|True,False|})

endsnippet

snippet pytorch:torchvision:load_segmentation_model "Torchvision Segmentation Model"
model = torchvision.models.segmentation.${1|fcn_resnet50,fcn_resnet101,deeplabv3_resnet50,deeplabv3_resnet101|}(pretrained=${2|False,True|}, progress=${3|True,False|})

endsnippet

snippet pytorch:torchvision:load_detection_model "Torchvision Detection Model"
model = torchvision.models.detection.${1|fasterrcnn_resnet50_fpn,maskrcnn_resnet50_fpn,keypointrcnn_resnet50_fpn|}(pretrained=${2|False,True|}, progress=${3|True,False|}, pretrained_backbone=${4|True,False|})

endsnippet

snippet pytorch:imports "PyTorch Imports"
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

endsnippet

snippet pytorch:device "Check Device"
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

endsnippet

snippet pytorch:optimizer "Optimizer"
optimizer = torch.optim.${1|Adadelta,Adagrad,Adam,SparseAdam,Adamax,ASGD,LBFGS,RMSprop,Rprop,SGD|}(${2:net}.parameters(), lr=${3:1e-2})

endsnippet

snippet pytorch:scheduler "Scheduler"
scheduler = torch.optim.lr_scheduler.${1|LambdaLR,StepLR,MultiStepLR,ExponentialLR,CosineAnnealingLR,ReduceLROnPlateau,CyclicLR|}(${2:optimizer})

endsnippet

snippet pytorch:dataset "Dataset"
class ${1:MyDataset}(torch.utils.data.Dataset):
	"""Some Information about ${1:MyDataset}"""
	def __init__(self):
		super(${1:MyDataset}, self).__init__()

	def __getitem__(self, index):
		return $2

	def __len__(self):
		return $3

endsnippet

snippet pytorch:dataloader "Dataloader"
dataloader = torch.utils.data.DataLoader(${1:dataset}, batch_size=${2:1}, shuffle=${3|False,True|})

endsnippet

snippet pytorch:loss_class "Classification Loss"
criterion = nn.${1|CrossEntropyLoss,NLLLoss,PoissonNLLLoss,BCELoss,BCEWithLogitsLoss,MarginRankingLoss,HingeEmbeddingLoss,MultiLabelMarginLoss,SoftMarginLoss,MultiLabelSoftMarginLoss,CosineEmbeddingLoss,MultiMarginLoss,TripletMarginLoss,CTCLoss|}()

endsnippet

snippet pytorch:loss_reg "Regression Loss"
criterion = nn.${1|L1Loss,MSELoss,KLDivLoss,SmoothL1Loss|}()

endsnippet

snippet pytorch:module "PyTorch Module"
class ${1:MyModule}(nn.Module):
	"""Some Information about ${1:MyModule}"""
	def __init__(self):
		super(${1:MyModule}, self).__init__()

	def forward(self, x):

		return x

endsnippet

snippet pytorch:container "Container"
layers = nn.${1|Sequential,ModuleList,ModuleDict,ParameterList,ParameterDict|}($2)

endsnippet

snippet pytorch:function "PyTorch Autograd Function"
class ${1:MyFunction}(torch.autograd.Function):
	"""Some Information about ${1:MyFunction}"""

	@staticmethod
	def forward(ctx, input):

		return

	@staticmethod
	def backward(ctx, grad_output)

		return

endsnippet

snippet pytorch:init "Initialize"
def init_weights(m):
	classname = m.__class__.__name__
	if classname.find('Linear') != -1 or classname.find('Bilinear') != -1:
		nn.init.${1|kaiming_uniform_(a=2\, mode='fan_in'\, nonlinearity='leaky_relu'\, ,kaiming_normal_(a=0\, mode='fan_in'\, nonlinearity='leaky_relu'\, ,xavier_uniform_(gain=1\, ,xavier_normal_(gain=1\, ,eye_(,orthogonal_(gain=1\, ,normal_(mean=0\, std=1\, ,uniform_(a=0\, b=1\, ,sparse_(sparsity=0.1\, std=0.01\, ,constant_(val=0.1\, ,zeros_(,ones_(|}tensor=m.weight)
		if m.bias: nn.init.${2|zeros(,uniform_(a=0\, b=1\, ,normal_(mean=0\, std=1\, ,ones_(,constant_(val=0.01\, |}tensor=m.bias)

	elif classname.find('Conv') != -1:
		nn.init.${3|kaiming_uniform_(a=2\, mode='fan_in'\, nonlinearity='leaky_relu'\, ,kaiming_normal_(a=0\, mode='fan_in'\, nonlinearity='leaky_relu'\, ,xavier_uniform_(gain=1\, ,xavier_normal_(gain=1\, ,dirac_(,orthogonal_(gain=1\, ,normal_(mean=0\, std=1\, ,uniform_(a=0\, b=1\, ,sparse_(sparsity=0.1\, std=0.01\, ,constant_(val=0.1\, ,zeros_(,ones_(|}tensor=m.weight)
		if m.bias: nn.init.${4|zeros(,normal_(mean=0\, std=1\, ,uniform_(a=0\, b=1\, ,ones_(,constant_(val=0.01\, |}tensor=m.bias)

	elif classname.find('BatchNorm') != -1 or classname.find('GroupNorm') != -1 or classname.find('LayerNorm') != -1:
		nn.init.${5|uniform_(a=0\, b=1\, ,normal_(mean=0\, std=1\, ,constant_(val=0.1\, ,zeros_(,ones_(|}tensor=m.weight)
		nn.init.${6|zeros(,normal_(mean=0\, std=1\, ,uniform_(a=0\, b=1\, ,ones_(,constant_(val=0.01\, |}tensor=m.bias)

	elif classname.find('Cell') != -1:
		nn.init.${7|xavier_uniform_(gain=1\, ,xavier_normal_(gain=1\, ,kaiming_uniform_(a=0\, mode='fan_in'\, nonlinearity='leaky_relu'\, ,kaiming_normal_(a=0\, mode='fan_in'\, nonlinearity='leaky_relu'\, ,eye_(,orthogonal_(gain=1\, ,normal_(mean=0\, std=1\, ,uniform_(a=0\, b=1\, ,sparse_(sparsity=0.1\, std=0.01\, ,constant_(val=0.1\, ,zeros_(,ones_(|}tensor=m.weight_hh)
		nn.init.${8|xavier_uniform_(gain=1\, ,xavier_normal_(gain=1\, ,kaiming_uniform_(a=0\, mode='fan_in'\, nonlinearity='leaky_relu'\, ,kaiming_normal_(a=0\, mode='fan_in'\, nonlinearity='leaky_relu'\, ,eye_(,orthogonal_(gain=1\, ,normal_(mean=0\, std=1\, ,uniform_(a=0\, b=1\, ,sparse_(sparsity=0.1\, std=0.01\, ,constant_(val=0.1\, ,zeros_(,ones_(|}tensor=m.weight_ih)
		nn.init.${9|ones_(,normal_(mean=0\, std=1\, ,uniform_(a=0\, b=1\, ,zeros(,constant_(val=0.01\, |}tensor=m.bias_hh)
		nn.init.${10|ones_(,normal_(mean=0\, std=1\, ,uniform_(a=0\, b=1\, ,zeros(,constant_(val=0.01\, |}tensor=m.bias_ih)

	elif classname.find('RNN') != -1 or classname.find('LSTM') != -1 or classname.find('GRU') != -1:
		for w in m.all_weights:
			nn.init.${11|xavier_uniform_(gain=1\, ,xavier_normal_(gain=1\, ,kaiming_uniform_(a=0\, mode='fan_in'\, nonlinearity='leaky_relu'\, ,kaiming_normal_(a=0\, mode='fan_in'\, nonlinearity='leaky_relu'\, ,eye_(,orthogonal_(gain=1\, ,normal_(mean=0\, std=1\, ,uniform_(a=0\, b=1\, ,sparse_(sparsity=0.1\, std=0.01\, ,constant_(val=0.1\, ,zeros_(,ones_(|}tensor=w[2].data)
			nn.init.${12|xavier_uniform_(gain=1\, ,xavier_normal_(gain=1\, ,kaiming_uniform_(a=0\, mode='fan_in'\, nonlinearity='leaky_relu'\, ,kaiming_normal_(a=0\, mode='fan_in'\, nonlinearity='leaky_relu'\, ,eye_(,orthogonal_(gain=1\, ,normal_(mean=0\, std=1\, ,uniform_(a=0\, b=1\, ,sparse_(sparsity=0.1\, std=0.01\, ,constant_(val=0.1\, ,zeros_(,ones_(|}tensor=w[3].data)
			nn.init.${13|ones_(,normal_(mean=0\, std=1\, ,uniform_(a=0\, b=1\, ,zeros(,constant_(val=0.01\, |}tensor=w[0].data)
			nn.init.${14|ones_(,normal_(mean=0\, std=1\, ,uniform_(a=0\, b=1\, ,zeros(,constant_(val=0.01\, |}tensor=w[1].data)

	if classname.find('Embedding') != -1:
		nn.init.${1|normal_(mean=0\, std=1\, ,xavier_uniform_(gain=1\, ,xavier_normal_(gain=1\, ,kaiming_uniform_(a=0\, mode='fan_in'\, nonlinearity='leaky_relu'\, ,kaiming_normal_(a=0\, mode='fan_in'\, nonlinearity='leaky_relu'\, ,eye_(,orthogonal_(gain=1\, ,uniform_(a=0\, b=1\, ,sparse_(sparsity=0.1\, std=0.01\, ,constant_(val=0.1\, ,zeros_(,ones_(|}tensor=m.weight)

${15:net}.apply(init_weights)

endsnippet

snippet pytorch:train "Train Loop"
# loop over the dataset multiple times
for epoch in range(${1:5}):
	running_loss = 0.0
	for i, data in enumerate(${2:trainloader}, 0):
		inputs, labels = data
		inputs, labels = inputs.to(${3:device}), labels.to(${3:device})

		# zero the parameter gradients
		${4:optimizer}.zero_grad()

		# forward + backward + optimize
		outputs = ${5:net}(inputs)
		loss = ${6:criterion}(outputs, labels)
		loss.backward()
		${4:optimizer}.step()

		running_loss += loss.item()

	print('Loss: {}'.format(running_loss)

print('Finished Training')

endsnippet

snippet pytorch:checkpoint "Model Checkpoint"
${1:model}.load_state_dict(${2|'path/to/model',torch.hub.load_state_dict_from_url('url')|})

endsnippet

snippet pytorch:github "GitHub Checkpoint"
${1:model} = torch.hub.load(github=${2:'pytorch/vision'}, model=${3:'resnet50'}, pretrained=${4|False,True|})

endsnippet

snippet pytorch:freeze "Freeze Layers"
for params in ${1:net}.parameters():
	params.require_grad = False

endsnippet

snippet pytorch:sampler "Sampler"
sampler = torch.utils.data.${1|Sampler(data_source),SequantialSample(data_source),RandomSampler(data_source),SubsetRandomSampler(indicies),WeightedRandomSampler(weights\, num_samples),BatchSampler(sampler\, batch_size\, drop_last),distributed.DistributedSampler(dataset)|}

endsnippet

snippet pytorch:unfreeze "Unfreeze Layers"
for params in ${1:net}.parameters():
	params.require_grad = True

endsnippet

snippet pytorch:layer:activation "Activation"
${1:nonlin} = nn.${2|ELU(alpha=1.\, inplace=False),Hardshrink(lambd=0.5),Hardtanh(min_val=-1\, max_val=1\, inplace=False\, min_value=None\, max_value=None),LeakyReLU(negative_slope=0.01\, inplace=False),LogSigmoid,PReLU(num_parameters=1\, init=0.25),ReLU(inplace=False),ReLU6(inplace=False),RReLU(lower=0.125\, upper=0.3333333333333333\, inplace=False),CELU(alpha=1.0\, inplace=False),SELU(inplace=False),Sigmoid,Softplus(beta=1\, threshold=20),Softshrink(lambd=0.5),Softsign,Tanh,Tanhshrink,Threshold(threshold\, value\, inplace=False),Softmin(dim=None),Softmax(dim=None),Softmax2d,LogSoftmax(dim=None),AdaptiveLogSoftmaxWithLogits(in_features\, n_classes\, cutoffs\, div_value=4.0\, head_bias=True)|}

endsnippet

snippet pytorch:layer:attention "Attention"
${1:mutli_attention} = nn.MultiheadAttention(embed_dim=$2, num_heads=$3)

endsnippet

snippet pytorch:layer:conv "Convolution Layer"
${1:conv} = nn.${2|Conv1d(in_channel\, out_channel\, groups=1\, bias=True\, ,Conv2d(in_channel\, out_channel\, groups=1\, bias=True\, ,Conv3d(in_channel\, out_channel\, groups=1\, bias=True\, ,ConvTranspose1d(in_channel\, out_channel\, groups=1\, bias=True\, out_padding=0\, dilation=1\, ,ConvTranspose2d/in_channel\, out_channel\, groups=1\, bias=True\, out_padding=0\, dilation=1\, ,ConvTranspose3d(in_channel\, out_channel\, groups=1\, bias=True\, out_padding=0\, dilation=1\, ,Unfold(dilation=1\, ,Fold(output_size\, |}kernel_size=2, padding=0, stride=1)

endsnippet

snippet pytorch:layer:pooling "Pooling Layer"
${1:pool} = nn.${2|MaxPool1d(kernel_size\, stride=None\, padding=0\, dilation=1\, return_indices=False\, ceil_mode=False),MaxPool2d(kernel_size\, stride=None\, padding=0\, dilation=1\, return_indices=False\, ceil_mode=False),MaxPool3d(kernel_size\, stride=None\, padding=0\, dilation=1\, return_indices=False\, ceil_mode=False),MaxUnpool1d(kernel_size\, stride=None\, padding=0),MaxUnpool2d(kernel_size\, stride=None\, padding=0),MaxUnpool3d(kernel_size\, stride=None\, padding=0),AvgPool1d(kernel_size\, stride=None\, padding=0\, ceil_mode=False\, count_include_pad=True),AvgPool2d(kernel_size\, stride=None\, padding=0\, ceil_mode=False\, count_include_pad=True),AvgPool3d(kernel_size\, stride=None\, padding=0\, ceil_mode=False\, count_include_pad=True),FractionalMaxPool2d(kernel_size\, output_size=None\, output_ratio=None\, return_indices=False\, random_samples=None),LPPool1d(norm_type\, kernel_size\, stride=None\, ceil_mode=False),LPPool2d(norm_type\, kernel_size\, stride=None\, ceil_mode=False),AdaptiveMaxPool1d(output_size\, return_indices=False),AdaptiveMaxPool2d(output_size\, return_indices=False),AdaptiveMaxPool3d(output_size\, return_indices=False),AdaptiveAvgPool1d(output_size),AdaptiveAvgPool2d(output_size),AdaptiveAvgPool3d(output_size)|}

endsnippet

snippet pytorch:layer:padding "Padding Layer"
${1:padding} = nn.${2|ReflectionPad1d(,ReflectionPad2d(,ReplicationPad1d(,ReplicationPad2d(,ReplicationPad3d(,ZeroPad2d(,ConstantPad1d(value=3.5\, ,ConstantPad2d(value=3.5\, ,ConstantPad3d(value=3.5\, |}padding=${3:(2,2)}

endsnippet

snippet pytorch:layer:recurrent "Recurrent Layer"
${1:recurrent} = nn.${2|RNN,LSTM,GRU,RNNCell,LSTMCell,GRUCell|}(${3:input_size}, ${4:hidden_size}, bias=${5:True})

endsnippet

snippet pytorch:layer:norm "Normalization Layer"
${1:norm} = nn.${2|BatchNorm1d(num_features\, eps=1e-5\, momentum=0.1\, affine=True\, track_running_stats=True),BatchNorm2d(num_features\, eps=1e-5\, momentum=0.1\, affine=True\, track_running_stats=True),BatchNorm3d(num_features\, eps=1e-5\, momentum=0.1\, affine=True\, track_running_stats=True),GroupNorm(num_groups\, num_channels\, eps=1e-5\, affine=True),SyncBatchNorm(num_features\, eps=1e-05\, momentum=0.1\, affine=True),InstanceNorm1d(num_features\, eps=1e-5\, momentum=0.1\, affine=False\, track_running_stats=False),InstanceNorm2d(num_features\, eps=1e-5\, momentum=0.1\, affine=False\, track_running_stats=False),InstanceNorm3d(num_features\, eps=1e-5\, momentum=0.1\, affine=False\, track_running_stats=False),LayerNorm(normalized_shape\, eps=1e-5\, elementwise_affine=True),LocalResponseNorm(size\, alpha=1e-4\, beta=0.75\, k=1)|}

endsnippet

snippet pytorch:layer:linear "Linear Layer"
${1:linear} = nn.${2|Identity(,Linear(in_feature\, ,Bilinear(in_features1\, in_features2\, |}out_features, bias=True)

endsnippet

snippet pytorch:layer:dropout "Dropout"
${1:drop} = nn.${2|Dropout,Dropout2d,Dropout3d,AlphaDropout|}(p=${3:0.5}, inplace=${4|False,True|})

endsnippet

snippet pytorch:layer:sparse "Sparse Layer"
${1:sparse} = nn.${2|Embedding,EmbeddingBag|}(${3:num_embeddings}, ${4:embedding_dim})

endsnippet

snippet pytorch:layer:vision "Vision Layer"
${1:vision} = nn.${2|PixelShuffle(upscale_factor),Upsample(size=None\, scale_factor=None\, mode='nearest'\, align_corners=None),UpsamplingNearest2d(size=None\, scale_factor=None),UpsamplingBilinear2d(size=None\, scale_factor=None)|}

endsnippet

snippet pytorch:layer:distance "Distance Layer"
${1:distance} = nn.${2|CosineSimilarity,PairwiseDistance|}()

endsnippet

snippet pytorch:F:activation "Activation Function"
F.${1|threshold(input\, threshold\, value\, inplace=False),relu(input\, inplace=False),relu6(input\, inplace=False),hardtanh(input\, min_val=-1.\, max_val=1.\, inplace=False),elu(input\, alpha=1.0\, inplace=False),selu(input\, inplace=False),celu(input\, alpha=1.\, inplace=False),leaky_relu(input\, negative_slope=0.01\, inplace=False),prelu(input\, weight),rrelu(input\, lower=1./8\, upper=1./3\, training=False\, inplace=False),glu(input\, dim=-1),logsigmoid(input),hardshrink(input\, lambd=0.5),tanhshrink(input),softsign(input),softplus(input\, beta=1\, threshold=20),softmin(input\, dim=None\, _stacklevel=3),softmax(input\, dim=None\, _stacklevel=3),softshrink(input\, lambd=0.5),gumbel_softmax(logits\, tau=1\, hard=False\, eps=1e-10),log_softmax(input\, dim=None\, _stacklevel=3),tanh(input),sigmoid(input)|}

endsnippet

snippet pytorch:F:conv "Convolution Function"
F.${1|conv1d,conv2d,conv3d,conv_transpose1d,conv_transpose2d,conv_transpose3d|}(${2:input}, ${3:weight}, bias=None, stride=1, padding=0)

endsnippet

snippet pytorch:F:pooling "Pooling Function"
F.${1|avg_pool1d(input\, kernel_size\, stride=None\, padding=0),avg_pool2d(input\, kernel_size\, stride=None\, padding=0),avg_pool3d(input\, kernel_size\, stride=None\, padding=0),max_pool1d(input\, kernel_size\, stride=None\, padding=0),max_pool2d(input\, kernel_size\, stride=None\, padding=0,max_pool3d(input\, kernel_size\, stride=None\, padding=0),max_unpool1d(input\, indices\, kernel_size\, stride=None\, padding=0),max_unpool2d(input\, indices\, kernel_size\, stride=None\, padding=0),max_unpool3d(input\, indices\, kernel_size\, stride=None\, padding=0),lp_pool1d(input\, norm_type\, kernel_size\, stride=None),lp_pool2d(input\, norm_type\, kernel_size\, stride=None),adaptive_max_pool1d(input\, output_size),adaptive_max_pool2d(input\, output_size),adaptive_max_pool3d(input\, output_size),adaptive_avg_pool1d(input\, output_size),adaptive_avg_pool2d(input\, output_size),adaptive_avg_pool3d(input\, output_size)|}

endsnippet

snippet pytorch:F:norm "Normalization Function"
F.${1|batch_norm(input\, running_mean\, running_var),instance_norm(input\, running_mean=None\, running_var=None),layer_norm(input\, normalized_shape),local_response_norm(input\, size),normalize(input)|}

endsnippet

snippet pytorch:F:linear "Linear Function"
F.${1|linear(input\, weight),bilinear(input1\, input2\, weight)|}

endsnippet

snippet pytorch:F:dropout "Dropout Function"
F.${1|dropout,dropout2d,dropout3d,alpha_dropout|}(${2:input}, p=${3:0.5})

endsnippet

snippet pytorch:F:sparse "Sparse Function"
F.${1|embedding,embedding_bag|}(${2:input}, ${3:weight})

endsnippet

snippet pytorch:F:one_hot "One Hot Encoding"
F.one_hot(${1:tensor}, num_classes=${2:0})

endsnippet

snippet pytorch:F:distance "Distance Function"
F.${1|pairwise_distance,cosine_similarity|}(${2:x1}, ${3:x2})

endsnippet

snippet pytorch:F:vision "Vision Function"
F.${1|pixel_shuffle(input\, upscale_factor),pad(input\, pad),interpolate(input\, size=None\, scale_factor=None\, mode='nearest'\, align_corners=None),grid_sample(input\, grid),affine_grid(theta\, size)|}

endsnippet

snippet pytorch:F:loss "Loss Function"
F.${1|cross_entropy,binary_cross_entropy,binary_cross_entropy_with_logits,poisson_nll_loss,hinge_embedding_loss,kl_div,l1_loss,smooth_l1_loss,mse_loss,multilabel_margin_loss,multilabel_soft_margin_loss,multi_margin_loss,nll_loss,soft_margin_loss|}(${2:input}, ${3:target})

endsnippet

snippet pytorch:layer:resnet:block "Resnet Basic Block"
class BasicBlock(nn.Module):
	# see https://pytorch.org/docs/0.4.0/_modules/torchvision/models/resnet.html
	def __init__(self, inplanes, planes, stride=1):
		super(BasicBlock, self).__init__()
		self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
		self.bn1 = nn.BatchNorm2d(planes)
		self.relu = nn.ReLU(inplace=True)
		self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False)
		self.bn2 = nn.BatchNorm2d(planes)

	def forward(self, x):
		residual = x
		out = self.conv1(x)
		out = self.bn1(out)
		out = self.relu(out)
		out = self.conv2(out)
		out = self.bn2(out)
		out += residual
		out = self.relu(out)
		return out

endsnippet

snippet pytorch:layer:resnet:bottleneck "Resnet Bottleneck Block"
class Bottleneck(nn.Module):
	# see https://pytorch.org/docs/0.4.0/_modules/torchvision/models/resnet.html
	def __init__(self, inplanes, planes, stride=1, downsample=None):
		super(Bottleneck, self).__init__()
		self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)
		self.bn1 = nn.BatchNorm2d(planes)
		self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
		self.bn2 = nn.BatchNorm2d(planes)
		self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)
		self.bn3 = nn.BatchNorm2d(planes * 4)
		self.relu = nn.ReLU(inplace=True)

	def forward(self, x):
		residual = x
		out = self.conv1(x)
		out = self.bn1(out)
		out = self.relu(out)
		out = self.conv2(out)
		out = self.bn2(out)
		out = self.relu(out)
		out = self.conv3(out)
		out = self.bn3(out)
		out += residual
		out = self.relu(out)
		return out

endsnippet

snippet pytorch:examples:imagenet "Imagenet Example"
import argparse
import os
import random
import shutil
import time
import warnings

import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.distributed as dist
import torch.optim
import torch.multiprocessing as mp
import torch.utils.data
import torch.utils.data.distributed
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision.models as models

model_names = sorted(name for name in models.__dict__
	if name.islower() and not name.startswith("__")
	and callable(models.__dict__[name]))

parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')
parser.add_argument('data', metavar='DIR',
					help='path to dataset')
parser.add_argument('-a', '--arch', metavar='ARCH', default='resnet18',
					choices=model_names,
					help='model architecture: ' +
						' | '.join(model_names) +
						' (default: resnet18)')
parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',
					help='number of data loading workers (default: 4)')
parser.add_argument('--epochs', default=90, type=int, metavar='N',
					help='number of total epochs to run')
parser.add_argument('--start-epoch', default=0, type=int, metavar='N',
					help='manual epoch number (useful on restarts)')
parser.add_argument('-b', '--batch-size', default=256, type=int,
					metavar='N',
					help='mini-batch size (default: 256), this is the total '
						 'batch size of all GPUs on the current node when '
						 'using Data Parallel or Distributed Data Parallel')
parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,
					metavar='LR', help='initial learning rate', dest='lr')
parser.add_argument('--momentum', default=0.9, type=float, metavar='M',
					help='momentum')
parser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,
					metavar='W', help='weight decay (default: 1e-4)',
					dest='weight_decay')
parser.add_argument('-p', '--print-freq', default=10, type=int,
					metavar='N', help='print frequency (default: 10)')
parser.add_argument('--resume', default='', type=str, metavar='PATH',
					help='path to latest checkpoint (default: none)')
parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',
					help='evaluate model on validation set')
parser.add_argument('--pretrained', dest='pretrained', action='store_true',
					help='use pre-trained model')
parser.add_argument('--world-size', default=-1, type=int,
					help='number of nodes for distributed training')
parser.add_argument('--rank', default=-1, type=int,
					help='node rank for distributed training')
parser.add_argument('--dist-url', default='tcp://224.66.41.62:23456', type=str,
					help='url used to set up distributed training')
parser.add_argument('--dist-backend', default='nccl', type=str,
					help='distributed backend')
parser.add_argument('--seed', default=None, type=int,
					help='seed for initializing training. ')
parser.add_argument('--gpu', default=None, type=int,
					help='GPU id to use.')
parser.add_argument('--multiprocessing-distributed', action='store_true',
					help='Use multi-processing distributed training to launch '
						 'N processes per node, which has N GPUs. This is the '
						 'fastest way to use PyTorch for either single node or '
						 'multi node data parallel training')

best_acc1 = 0


def main():
	args = parser.parse_args()

	if args.seed is not None:
		random.seed(args.seed)
		torch.manual_seed(args.seed)
		cudnn.deterministic = True
		warnings.warn('You have chosen to seed training. '
					  'This will turn on the CUDNN deterministic setting, '
					  'which can slow down your training considerably! '
					  'You may see unexpected behavior when restarting '
					  'from checkpoints.')

	if args.gpu is not None:
		warnings.warn('You have chosen a specific GPU. This will completely '
					  'disable data parallelism.')

	if args.dist_url == "env://" and args.world_size == -1:
		args.world_size = int(os.environ["WORLD_SIZE"])

	args.distributed = args.world_size > 1 or args.multiprocessing_distributed

	ngpus_per_node = torch.cuda.device_count()
	if args.multiprocessing_distributed:
		# Since we have ngpus_per_node processes per node, the total world_size
		# needs to be adjusted accordingly
		args.world_size = ngpus_per_node * args.world_size
		# Use torch.multiprocessing.spawn to launch distributed processes: the
		# main_worker process function
		mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))
	else:
		# Simply call main_worker function
		main_worker(args.gpu, ngpus_per_node, args)


def main_worker(gpu, ngpus_per_node, args):
	global best_acc1
	args.gpu = gpu

	if args.gpu is not None:
		print("Use GPU: {} for training".format(args.gpu))

	if args.distributed:
		if args.dist_url == "env://" and args.rank == -1:
			args.rank = int(os.environ["RANK"])
		if args.multiprocessing_distributed:
			# For multiprocessing distributed training, rank needs to be the
			# global rank among all the processes
			args.rank = args.rank * ngpus_per_node + gpu
		dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
								world_size=args.world_size, rank=args.rank)
	# create model
	if args.pretrained:
		print("=> using pre-trained model '{}'".format(args.arch))
		model = models.__dict__[args.arch](pretrained=True)
	else:
		print("=> creating model '{}'".format(args.arch))
		model = models.__dict__[args.arch]()

	if args.distributed:
		# For multiprocessing distributed, DistributedDataParallel constructor
		# should always set the single device scope, otherwise,
		# DistributedDataParallel will use all available devices.
		if args.gpu is not None:
			torch.cuda.set_device(args.gpu)
			model.cuda(args.gpu)
			# When using a single GPU per process and per
			# DistributedDataParallel, we need to divide the batch size
			# ourselves based on the total number of GPUs we have
			args.batch_size = int(args.batch_size / ngpus_per_node)
			args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)
			model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])
		else:
			model.cuda()
			# DistributedDataParallel will divide and allocate batch_size to all
			# available GPUs if device_ids are not set
			model = torch.nn.parallel.DistributedDataParallel(model)
	elif args.gpu is not None:
		torch.cuda.set_device(args.gpu)
		model = model.cuda(args.gpu)
	else:
		# DataParallel will divide and allocate batch_size to all available GPUs
		if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):
			model.features = torch.nn.DataParallel(model.features)
			model.cuda()
		else:
			model = torch.nn.DataParallel(model).cuda()

	# define loss function (criterion) and optimizer
	criterion = nn.CrossEntropyLoss().cuda(args.gpu)

	optimizer = torch.optim.SGD(model.parameters(), args.lr,
								momentum=args.momentum,
								weight_decay=args.weight_decay)

	# optionally resume from a checkpoint
	if args.resume:
		if os.path.isfile(args.resume):
			print("=> loading checkpoint '{}'".format(args.resume))
			checkpoint = torch.load(args.resume)
			args.start_epoch = checkpoint['epoch']
			best_acc1 = checkpoint['best_acc1']
			if args.gpu is not None:
				# best_acc1 may be from a checkpoint from a different GPU
				best_acc1 = best_acc1.to(args.gpu)
			model.load_state_dict(checkpoint['state_dict'])
			optimizer.load_state_dict(checkpoint['optimizer'])
			print("=> loaded checkpoint '{}' (epoch {})"
				  .format(args.resume, checkpoint['epoch']))
		else:
			print("=> no checkpoint found at '{}'".format(args.resume))

	cudnn.benchmark = True

	# Data loading code
	traindir = os.path.join(args.data, 'train')
	valdir = os.path.join(args.data, 'val')
	normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
								     std=[0.229, 0.224, 0.225])

	train_dataset = datasets.ImageFolder(
		traindir,
		transforms.Compose([
			transforms.RandomResizedCrop(224),
			transforms.RandomHorizontalFlip(),
			transforms.ToTensor(),
			normalize,
		]))

	if args.distributed:
		train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
	else:
		train_sampler = None

	train_loader = torch.utils.data.DataLoader(
		train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),
		num_workers=args.workers, pin_memory=True, sampler=train_sampler)

	val_loader = torch.utils.data.DataLoader(
		datasets.ImageFolder(valdir, transforms.Compose([
			transforms.Resize(256),
			transforms.CenterCrop(224),
			transforms.ToTensor(),
			normalize,
		])),
		batch_size=args.batch_size, shuffle=False,
		num_workers=args.workers, pin_memory=True)

	if args.evaluate:
		validate(val_loader, model, criterion, args)
		return

	for epoch in range(args.start_epoch, args.epochs):
		if args.distributed:
			train_sampler.set_epoch(epoch)
		adjust_learning_rate(optimizer, epoch, args)

		# train for one epoch
		train(train_loader, model, criterion, optimizer, epoch, args)

		# evaluate on validation set
		acc1 = validate(val_loader, model, criterion, args)

		# remember best acc@1 and save checkpoint
		is_best = acc1 > best_acc1
		best_acc1 = max(acc1, best_acc1)

		if not args.multiprocessing_distributed or (args.multiprocessing_distributed
				and args.rank % ngpus_per_node == 0):
			save_checkpoint({
				'epoch': epoch + 1,
				'arch': args.arch,
				'state_dict': model.state_dict(),
				'best_acc1': best_acc1,
				'optimizer' : optimizer.state_dict(),
			}, is_best)


def train(train_loader, model, criterion, optimizer, epoch, args):
	batch_time = AverageMeter('Time', ':6.3f')
	data_time = AverageMeter('Data', ':6.3f')
	losses = AverageMeter('Loss', ':.4e')
	top1 = AverageMeter('Acc@1', ':6.2f')
	top5 = AverageMeter('Acc@5', ':6.2f')
	progress = ProgressMeter(
		len(train_loader),
		[batch_time, data_time, losses, top1, top5],
		prefix="Epoch: [{}]".format(epoch))

	# switch to train mode
	model.train()

	end = time.time()
	for i, (images, target) in enumerate(train_loader):
		# measure data loading time
		data_time.update(time.time() - end)

		if args.gpu is not None:
			images = images.cuda(args.gpu, non_blocking=True)
		target = target.cuda(args.gpu, non_blocking=True)

		# compute output
		output = model(images)
		loss = criterion(output, target)

		# measure accuracy and record loss
		acc1, acc5 = accuracy(output, target, topk=(1, 5))
		losses.update(loss.item(), images.size(0))
		top1.update(acc1[0], images.size(0))
		top5.update(acc5[0], images.size(0))

		# compute gradient and do SGD step
		optimizer.zero_grad()
		loss.backward()
		optimizer.step()

		# measure elapsed time
		batch_time.update(time.time() - end)
		end = time.time()

		if i % args.print_freq == 0:
			progress.display(i)


def validate(val_loader, model, criterion, args):
	batch_time = AverageMeter('Time', ':6.3f')
	losses = AverageMeter('Loss', ':.4e')
	top1 = AverageMeter('Acc@1', ':6.2f')
	top5 = AverageMeter('Acc@5', ':6.2f')
	progress = ProgressMeter(
		len(val_loader),
		[batch_time, losses, top1, top5],
		prefix='Test: ')

	# switch to evaluate mode
	model.eval()

	with torch.no_grad():
		end = time.time()
		for i, (images, target) in enumerate(val_loader):
			if args.gpu is not None:
				images = images.cuda(args.gpu, non_blocking=True)
			target = target.cuda(args.gpu, non_blocking=True)

			# compute output
			output = model(images)
			loss = criterion(output, target)

			# measure accuracy and record loss
			acc1, acc5 = accuracy(output, target, topk=(1, 5))
			losses.update(loss.item(), images.size(0))
			top1.update(acc1[0], images.size(0))
			top5.update(acc5[0], images.size(0))

			# measure elapsed time
			batch_time.update(time.time() - end)
			end = time.time()

			if i % args.print_freq == 0:
				progress.display(i)

		# TODO: this should also be done with the ProgressMeter
		print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'
			  .format(top1=top1, top5=top5))

	return top1.avg


def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):
	torch.save(state, filename)
	if is_best:
		shutil.copyfile(filename, 'model_best.pth.tar')


class AverageMeter(object):
	"""Computes and stores the average and current value"""
	def __init__(self, name, fmt=':f'):
		self.name = name
		self.fmt = fmt
		self.reset()

	def reset(self):
		self.val = 0
		self.avg = 0
		self.sum = 0
		self.count = 0

	def update(self, val, n=1):
		self.val = val
		self.sum += val * n
		self.count += n
		self.avg = self.sum / self.count

	def __str__(self):
		fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'
		return fmtstr.format(**self.__dict__)


class ProgressMeter(object):
	def __init__(self, num_batches, meters, prefix=""):
		self.batch_fmtstr = self._get_batch_fmtstr(num_batches)
		self.meters = meters
		self.prefix = prefix

	def display(self, batch):
		entries = [self.prefix + self.batch_fmtstr.format(batch)]
		entries += [str(meter) for meter in self.meters]
		print('\t'.join(entries))

	def _get_batch_fmtstr(self, num_batches):
		num_digits = len(str(num_batches // 1))
		fmt = '{:' + str(num_digits) + 'd}'
		return '[' + fmt + '/' + fmt.format(num_batches) + ']'


def adjust_learning_rate(optimizer, epoch, args):
	"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""
	lr = args.lr * (0.1 ** (epoch // 30))
	for param_group in optimizer.param_groups:
		param_group['lr'] = lr


def accuracy(output, target, topk=(1,)):
	"""Computes the accuracy over the k top predictions for the specified values of k"""
	with torch.no_grad():
		maxk = max(topk)
		batch_size = target.size(0)

		_, pred = output.topk(maxk, 1, True, True)
		pred = pred.t()
		correct = pred.eq(target.view(1, -1).expand_as(pred))

		res = []
		for k in topk:
			correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)
			res.append(correct_k.mul_(100.0 / batch_size))
		return res


if __name__ == '__main__':
	main()

endsnippet

snippet pytorch:examples:mnist "Mnist Example"
from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms


class Net(nn.Module):
	def __init__(self):
		super(Net, self).__init__()
		self.conv1 = nn.Conv2d(1, 20, 5, 1)
		self.conv2 = nn.Conv2d(20, 50, 5, 1)
		self.fc1 = nn.Linear(4*4*50, 500)
		self.fc2 = nn.Linear(500, 10)

	def forward(self, x):
		x = F.relu(self.conv1(x))
		x = F.max_pool2d(x, 2, 2)
		x = F.relu(self.conv2(x))
		x = F.max_pool2d(x, 2, 2)
		x = x.view(-1, 4*4*50)
		x = F.relu(self.fc1(x))
		x = self.fc2(x)
		return F.log_softmax(x, dim=1)

def train(args, model, device, train_loader, optimizer, epoch):
	model.train()
	for batch_idx, (data, target) in enumerate(train_loader):
		data, target = data.to(device), target.to(device)
		optimizer.zero_grad()
		output = model(data)
		loss = F.nll_loss(output, target)
		loss.backward()
		optimizer.step()
		if batch_idx % args.log_interval == 0:
			print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
				epoch, batch_idx * len(data), len(train_loader.dataset),
				100. * batch_idx / len(train_loader), loss.item()))

def test(args, model, device, test_loader):
	model.eval()
	test_loss = 0
	correct = 0
	with torch.no_grad():
		for data, target in test_loader:
			data, target = data.to(device), target.to(device)
			output = model(data)
			test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss
			pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability
			correct += pred.eq(target.view_as(pred)).sum().item()

	test_loss /= len(test_loader.dataset)

	print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
		test_loss, correct, len(test_loader.dataset),
		100. * correct / len(test_loader.dataset)))

def main():
	# Training settings
	parser = argparse.ArgumentParser(description='PyTorch MNIST Example')
	parser.add_argument('--batch-size', type=int, default=64, metavar='N',
						help='input batch size for training (default: 64)')
	parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
						help='input batch size for testing (default: 1000)')
	parser.add_argument('--epochs', type=int, default=10, metavar='N',
						help='number of epochs to train (default: 10)')
	parser.add_argument('--lr', type=float, default=0.01, metavar='LR',
						help='learning rate (default: 0.01)')
	parser.add_argument('--momentum', type=float, default=0.5, metavar='M',
						help='SGD momentum (default: 0.5)')
	parser.add_argument('--no-cuda', action='store_true', default=False,
						help='disables CUDA training')
	parser.add_argument('--seed', type=int, default=1, metavar='S',
						help='random seed (default: 1)')
	parser.add_argument('--log-interval', type=int, default=10, metavar='N',
						help='how many batches to wait before logging training status')

	parser.add_argument('--save-model', action='store_true', default=False,
						help='For Saving the current Model')
	args = parser.parse_args()
	use_cuda = not args.no_cuda and torch.cuda.is_available()

	torch.manual_seed(args.seed)

	device = torch.device("cuda" if use_cuda else "cpu")

	kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}
	train_loader = torch.utils.data.DataLoader(
		datasets.MNIST('../data', train=True, download=True,
					   transform=transforms.Compose([
						   transforms.ToTensor(),
						   transforms.Normalize((0.1307,), (0.3081,))
					   ])),
		batch_size=args.batch_size, shuffle=True, **kwargs)
	test_loader = torch.utils.data.DataLoader(
		datasets.MNIST('../data', train=False, transform=transforms.Compose([
						   transforms.ToTensor(),
						   transforms.Normalize((0.1307,), (0.3081,))
					   ])),
		batch_size=args.test_batch_size, shuffle=True, **kwargs)


	model = Net().to(device)
	optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)

	for epoch in range(1, args.epochs + 1):
		train(args, model, device, train_loader, optimizer, epoch)
		test(args, model, device, test_loader)

	if (args.save_model):
		torch.save(model.state_dict(),"mnist_cnn.pt")

if __name__ == '__main__':
	main()

endsnippet

snippet pytorch:examples:vpg "RL Example VPG"
import argparse
import gym
import numpy as np
from itertools import count

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical


parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')
parser.add_argument('--gamma', type=float, default=0.99, metavar='G',
					help='discount factor (default: 0.99)')
parser.add_argument('--seed', type=int, default=543, metavar='N',
					help='random seed (default: 543)')
parser.add_argument('--render', action='store_true',
					help='render the environment')
parser.add_argument('--log-interval', type=int, default=10, metavar='N',
					help='interval between training status logs (default: 10)')
args = parser.parse_args()


env = gym.make('CartPole-v1')
env.seed(args.seed)
torch.manual_seed(args.seed)


class Policy(nn.Module):
	def __init__(self):
		super(Policy, self).__init__()
		self.affine1 = nn.Linear(4, 128)
		self.dropout = nn.Dropout(p=0.6)
		self.affine2 = nn.Linear(128, 2)

		self.saved_log_probs = []
		self.rewards = []

	def forward(self, x):
		x = self.affine1(x)
		x = self.dropout(x)
		x = F.relu(x)
		action_scores = self.affine2(x)
		return F.softmax(action_scores, dim=1)


policy = Policy()
optimizer = optim.Adam(policy.parameters(), lr=1e-2)
eps = np.finfo(np.float32).eps.item()


def select_action(state):
	state = torch.from_numpy(state).float().unsqueeze(0)
	probs = policy(state)
	m = Categorical(probs)
	action = m.sample()
	policy.saved_log_probs.append(m.log_prob(action))
	return action.item()


def finish_episode():
	R = 0
	policy_loss = []
	returns = []
	for r in policy.rewards[::-1]:
		R = r + args.gamma * R
		returns.insert(0, R)
	returns = torch.tensor(returns)
	returns = (returns - returns.mean()) / (returns.std() + eps)
	for log_prob, R in zip(policy.saved_log_probs, returns):
		policy_loss.append(-log_prob * R)
	optimizer.zero_grad()
	policy_loss = torch.cat(policy_loss).sum()
	policy_loss.backward()
	optimizer.step()
	del policy.rewards[:]
	del policy.saved_log_probs[:]


def main():
	running_reward = 10
	for i_episode in count(1):
		state, ep_reward = env.reset(), 0
		for t in range(1, 10000):  # Don't infinite loop while learning
			action = select_action(state)
			state, reward, done, _ = env.step(action)
			if args.render:
				env.render()
			policy.rewards.append(reward)
			ep_reward += reward
			if done:
				break

		running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward
		finish_episode()
		if i_episode % args.log_interval == 0:
			print('Episode {}\tLast reward: {:.2f}\tAverage reward: {:.2f}'.format(
				  i_episode, ep_reward, running_reward))
		if running_reward > env.spec.reward_threshold:
			print("Solved! Running reward is now {} and "
				  "the last episode runs to {} time steps!".format(running_reward, t))
			break


if __name__ == '__main__':
	main()

endsnippet

snippet pytorch:examples:ac "RL Example Actor-Critic"
import argparse
import gym
import numpy as np
from itertools import count
from collections import namedtuple

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical

# Cart Pole

parser = argparse.ArgumentParser(description='PyTorch actor-critic example')
parser.add_argument('--gamma', type=float, default=0.99, metavar='G',
					help='discount factor (default: 0.99)')
parser.add_argument('--seed', type=int, default=543, metavar='N',
					help='random seed (default: 543)')
parser.add_argument('--render', action='store_true',
					help='render the environment')
parser.add_argument('--log-interval', type=int, default=10, metavar='N',
					help='interval between training status logs (default: 10)')
args = parser.parse_args()


env = gym.make('CartPole-v0')
env.seed(args.seed)
torch.manual_seed(args.seed)


SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])


class Policy(nn.Module):
	"""
	implements both actor and critic in one model
	"""
	def __init__(self):
		super(Policy, self).__init__()
		self.affine1 = nn.Linear(4, 128)

		# actor's layer
		self.action_head = nn.Linear(128, 2)

		# critic's layer
		self.value_head = nn.Linear(128, 1)

		# action & reward buffer
		self.saved_actions = []
		self.rewards = []

	def forward(self, x):
		"""
		forward of both actor and critic
		"""
		x = F.relu(self.affine1(x))

		# actor: choses action to take from state s_t
		# by returning probability of each action
		action_prob = F.softmax(self.action_head(x), dim=-1)

		# critic: evaluates being in the state s_t
		state_values = self.value_head(x)

		# return values for both actor and critic as a tupel of 2 values:
		# 1. a list with the probability of each action over the action space
		# 2. the value from state s_t
		return action_prob, state_values


model = Policy()
optimizer = optim.Adam(model.parameters(), lr=3e-2)
eps = np.finfo(np.float32).eps.item()


def select_action(state):
	state = torch.from_numpy(state).float()
	probs, state_value = model(state)

	# create a categorical distribution over the list of probabilities of actions
	m = Categorical(probs)

	# and sample an action using the distribution
	action = m.sample()

	# save to action buffer
	model.saved_actions.append(SavedAction(m.log_prob(action), state_value))

	# the action to take (left or right)
	return action.item()


def finish_episode():
	"""
	Training code. Calcultes actor and critic loss and performs backprop.
	"""
	R = 0
	saved_actions = model.saved_actions
	policy_losses = [] # list to save actor (policy) loss
	value_losses = [] # list to save critic (value) loss
	returns = [] # list to save the true values

	# calculate the true value using rewards returned from the environment
	for r in model.rewards[::-1]:
		# calculate the discounted value
		R = r + args.gamma * R
		returns.insert(0, R)

	returns = torch.tensor(returns)
	returns = (returns - returns.mean()) / (returns.std() + eps)

	for (log_prob, value), R in zip(saved_actions, returns):
		advantage = R - value.item()

		# calculate actor (policy) loss
		policy_losses.append(-log_prob * advantage)

		# calculate critic (value) loss using L1 smooth loss
		value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))

	# reset gradients
	optimizer.zero_grad()

	# sum up all the values of policy_losses and value_losses
	loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()

	# perform backprop
	loss.backward()
	optimizer.step()

	# reset rewards and action buffer
	del model.rewards[:]
	del model.saved_actions[:]


def main():
	running_reward = 10

	# run inifinitely many episodes
	for i_episode in count(1):

		# reset environment and episode reward
		state = env.reset()
		ep_reward = 0

		# for each episode, only run 9999 steps so that we don't
		# infinite loop while learning
		for t in range(1, 10000):

			# select action from policy
			action = select_action(state)

			# take the action
			state, reward, done, _ = env.step(action)

			if args.render:
				env.render()

			model.rewards.append(reward)
			ep_reward += reward
			if done:
				break

		# update cumulative reward
		running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward

		# perform backprop
		finish_episode()

		# log results
		if i_episode % args.log_interval == 0:
			print('Episode {}\tLast reward: {:.2f}\tAverage reward: {:.2f}'.format(
				  i_episode, ep_reward, running_reward))

		# check if we have "solved" the cart pole problem
		if running_reward > env.spec.reward_threshold:
			print("Solved! Running reward is now {} and "
				  "the last episode runs to {} time steps!".format(running_reward, t))
			break


if __name__ == '__main__':
	main()

endsnippet

snippet pytorch:examples:dcgan "DCGAN Example"
from __future__ import print_function
import argparse
import os
import random
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils


parser = argparse.ArgumentParser()
parser.add_argument('--dataset', required=True, help='cifar10 | lsun | mnist |imagenet | folder | lfw | fake')
parser.add_argument('--dataroot', required=True, help='path to dataset')
parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)
parser.add_argument('--batchSize', type=int, default=64, help='input batch size')
parser.add_argument('--imageSize', type=int, default=64, help='the height / width of the input image to network')
parser.add_argument('--nz', type=int, default=100, help='size of the latent z vector')
parser.add_argument('--ngf', type=int, default=64)
parser.add_argument('--ndf', type=int, default=64)
parser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')
parser.add_argument('--lr', type=float, default=0.0002, help='learning rate, default=0.0002')
parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')
parser.add_argument('--cuda', action='store_true', help='enables cuda')
parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')
parser.add_argument('--netG', default='', help="path to netG (to continue training)")
parser.add_argument('--netD', default='', help="path to netD (to continue training)")
parser.add_argument('--outf', default='.', help='folder to output images and model checkpoints')
parser.add_argument('--manualSeed', type=int, help='manual seed')
parser.add_argument('--classes', default='bedroom', help='comma separated list of classes for the lsun data set')

opt = parser.parse_args()
print(opt)

try:
	os.makedirs(opt.outf)
except OSError:
	pass

if opt.manualSeed is None:
	opt.manualSeed = random.randint(1, 10000)
print("Random Seed: ", opt.manualSeed)
random.seed(opt.manualSeed)
torch.manual_seed(opt.manualSeed)

cudnn.benchmark = True

if torch.cuda.is_available() and not opt.cuda:
	print("WARNING: You have a CUDA device, so you should probably run with --cuda")

if opt.dataset in ['imagenet', 'folder', 'lfw']:
	# folder dataset
	dataset = dset.ImageFolder(root=opt.dataroot,
							   transform=transforms.Compose([
								   transforms.Resize(opt.imageSize),
								   transforms.CenterCrop(opt.imageSize),
								   transforms.ToTensor(),
								   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
							   ]))
	nc=3
elif opt.dataset == 'lsun':
	classes = [ c + '_train' for c in opt.classes.split(',')]
	dataset = dset.LSUN(root=opt.dataroot, classes=classes,
						transform=transforms.Compose([
							transforms.Resize(opt.imageSize),
							transforms.CenterCrop(opt.imageSize),
							transforms.ToTensor(),
							transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
						]))
	nc=3
elif opt.dataset == 'cifar10':
	dataset = dset.CIFAR10(root=opt.dataroot, download=True,
						   transform=transforms.Compose([
							   transforms.Resize(opt.imageSize),
							   transforms.ToTensor(),
							   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
						   ]))
	nc=3

elif opt.dataset == 'mnist':
		dataset = dset.MNIST(root=opt.dataroot, download=True,
						   transform=transforms.Compose([
							   transforms.Resize(opt.imageSize),
							   transforms.ToTensor(),
							   transforms.Normalize((0.5,), (0.5,)),
						   ]))
		nc=1

elif opt.dataset == 'fake':
	dataset = dset.FakeData(image_size=(3, opt.imageSize, opt.imageSize),
							transform=transforms.ToTensor())
	nc=3

assert dataset
dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,
								         shuffle=True, num_workers=int(opt.workers))

device = torch.device("cuda:0" if opt.cuda else "cpu")
ngpu = int(opt.ngpu)
nz = int(opt.nz)
ngf = int(opt.ngf)
ndf = int(opt.ndf)


# custom weights initialization called on netG and netD
def weights_init(m):
	classname = m.__class__.__name__
	if classname.find('Conv') != -1:
		m.weight.data.normal_(0.0, 0.02)
	elif classname.find('BatchNorm') != -1:
		m.weight.data.normal_(1.0, 0.02)
		m.bias.data.fill_(0)


class Generator(nn.Module):
	def __init__(self, ngpu):
		super(Generator, self).__init__()
		self.ngpu = ngpu
		self.main = nn.Sequential(
			# input is Z, going into a convolution
			nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),
			nn.BatchNorm2d(ngf * 8),
			nn.ReLU(True),
			# state size. (ngf*8) x 4 x 4
			nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
			nn.BatchNorm2d(ngf * 4),
			nn.ReLU(True),
			# state size. (ngf*4) x 8 x 8
			nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
			nn.BatchNorm2d(ngf * 2),
			nn.ReLU(True),
			# state size. (ngf*2) x 16 x 16
			nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),
			nn.BatchNorm2d(ngf),
			nn.ReLU(True),
			# state size. (ngf) x 32 x 32
			nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),
			nn.Tanh()
			# state size. (nc) x 64 x 64
		)

	def forward(self, input):
		if input.is_cuda and self.ngpu > 1:
			output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))
		else:
			output = self.main(input)
		return output


netG = Generator(ngpu).to(device)
netG.apply(weights_init)
if opt.netG != '':
	netG.load_state_dict(torch.load(opt.netG))
print(netG)


class Discriminator(nn.Module):
	def __init__(self, ngpu):
		super(Discriminator, self).__init__()
		self.ngpu = ngpu
		self.main = nn.Sequential(
			# input is (nc) x 64 x 64
			nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),
			nn.LeakyReLU(0.2, inplace=True),
			# state size. (ndf) x 32 x 32
			nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
			nn.BatchNorm2d(ndf * 2),
			nn.LeakyReLU(0.2, inplace=True),
			# state size. (ndf*2) x 16 x 16
			nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
			nn.BatchNorm2d(ndf * 4),
			nn.LeakyReLU(0.2, inplace=True),
			# state size. (ndf*4) x 8 x 8
			nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),
			nn.BatchNorm2d(ndf * 8),
			nn.LeakyReLU(0.2, inplace=True),
			# state size. (ndf*8) x 4 x 4
			nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),
			nn.Sigmoid()
		)

	def forward(self, input):
		if input.is_cuda and self.ngpu > 1:
			output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))
		else:
			output = self.main(input)

		return output.view(-1, 1).squeeze(1)


netD = Discriminator(ngpu).to(device)
netD.apply(weights_init)
if opt.netD != '':
	netD.load_state_dict(torch.load(opt.netD))
print(netD)

criterion = nn.BCELoss()

fixed_noise = torch.randn(opt.batchSize, nz, 1, 1, device=device)
real_label = 1
fake_label = 0

# setup optimizer
optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))
optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))

for epoch in range(opt.niter):
	for i, data in enumerate(dataloader, 0):
		############################
		# (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))
		###########################
		# train with real
		netD.zero_grad()
		real_cpu = data[0].to(device)
		batch_size = real_cpu.size(0)
		label = torch.full((batch_size,), real_label, device=device)

		output = netD(real_cpu)
		errD_real = criterion(output, label)
		errD_real.backward()
		D_x = output.mean().item()

		# train with fake
		noise = torch.randn(batch_size, nz, 1, 1, device=device)
		fake = netG(noise)
		label.fill_(fake_label)
		output = netD(fake.detach())
		errD_fake = criterion(output, label)
		errD_fake.backward()
		D_G_z1 = output.mean().item()
		errD = errD_real + errD_fake
		optimizerD.step()

		############################
		# (2) Update G network: maximize log(D(G(z)))
		###########################
		netG.zero_grad()
		label.fill_(real_label)  # fake labels are real for generator cost
		output = netD(fake)
		errG = criterion(output, label)
		errG.backward()
		D_G_z2 = output.mean().item()
		optimizerG.step()

		print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'
			  % (epoch, opt.niter, i, len(dataloader),
				 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))
		if i % 100 == 0:
			vutils.save_image(real_cpu,
					'%s/real_samples.png' % opt.outf,
					normalize=True)
			fake = netG(fixed_noise)
			vutils.save_image(fake.detach(),
					'%s/fake_samples_epoch_%03d.png' % (opt.outf, epoch),
					normalize=True)

	# do checkpointing
	torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (opt.outf, epoch))
	torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (opt.outf, epoch))

endsnippet

snippet pytorch:examples:vae "Variational Autoencoder Example"
from __future__ import print_function
import argparse
import torch
import torch.utils.data
from torch import nn, optim
from torch.nn import functional as F
from torchvision import datasets, transforms
from torchvision.utils import save_image


parser = argparse.ArgumentParser(description='VAE MNIST Example')
parser.add_argument('--batch-size', type=int, default=128, metavar='N',
					help='input batch size for training (default: 128)')
parser.add_argument('--epochs', type=int, default=10, metavar='N',
					help='number of epochs to train (default: 10)')
parser.add_argument('--no-cuda', action='store_true', default=False,
					help='enables CUDA training')
parser.add_argument('--seed', type=int, default=1, metavar='S',
					help='random seed (default: 1)')
parser.add_argument('--log-interval', type=int, default=10, metavar='N',
					help='how many batches to wait before logging training status')
args = parser.parse_args()
args.cuda = not args.no_cuda and torch.cuda.is_available()

torch.manual_seed(args.seed)

device = torch.device("cuda" if args.cuda else "cpu")

kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}
train_loader = torch.utils.data.DataLoader(
	datasets.MNIST('../data', train=True, download=True,
				   transform=transforms.ToTensor()),
	batch_size=args.batch_size, shuffle=True, **kwargs)
test_loader = torch.utils.data.DataLoader(
	datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),
	batch_size=args.batch_size, shuffle=True, **kwargs)


class VAE(nn.Module):
	def __init__(self):
		super(VAE, self).__init__()

		self.fc1 = nn.Linear(784, 400)
		self.fc21 = nn.Linear(400, 20)
		self.fc22 = nn.Linear(400, 20)
		self.fc3 = nn.Linear(20, 400)
		self.fc4 = nn.Linear(400, 784)

	def encode(self, x):
		h1 = F.relu(self.fc1(x))
		return self.fc21(h1), self.fc22(h1)

	def reparameterize(self, mu, logvar):
		std = torch.exp(0.5*logvar)
		eps = torch.randn_like(std)
		return mu + eps*std

	def decode(self, z):
		h3 = F.relu(self.fc3(z))
		return torch.sigmoid(self.fc4(h3))

	def forward(self, x):
		mu, logvar = self.encode(x.view(-1, 784))
		z = self.reparameterize(mu, logvar)
		return self.decode(z), mu, logvar


model = VAE().to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)


# Reconstruction + KL divergence losses summed over all elements and batch
def loss_function(recon_x, x, mu, logvar):
	BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')

	# see Appendix B from VAE paper:
	# Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014
	# https://arxiv.org/abs/1312.6114
	# 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
	KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

	return BCE + KLD


def train(epoch):
	model.train()
	train_loss = 0
	for batch_idx, (data, _) in enumerate(train_loader):
		data = data.to(device)
		optimizer.zero_grad()
		recon_batch, mu, logvar = model(data)
		loss = loss_function(recon_batch, data, mu, logvar)
		loss.backward()
		train_loss += loss.item()
		optimizer.step()
		if batch_idx % args.log_interval == 0:
			print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
				epoch, batch_idx * len(data), len(train_loader.dataset),
				100. * batch_idx / len(train_loader),
				loss.item() / len(data)))

	print('====> Epoch: {} Average loss: {:.4f}'.format(
		  epoch, train_loss / len(train_loader.dataset)))


def test(epoch):
	model.eval()
	test_loss = 0
	with torch.no_grad():
		for i, (data, _) in enumerate(test_loader):
			data = data.to(device)
			recon_batch, mu, logvar = model(data)
			test_loss += loss_function(recon_batch, data, mu, logvar).item()
			if i == 0:
				n = min(data.size(0), 8)
				comparison = torch.cat([data[:n],
								      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])
				save_image(comparison.cpu(),
						 'results/reconstruction_' + str(epoch) + '.png', nrow=n)

	test_loss /= len(test_loader.dataset)
	print('====> Test set loss: {:.4f}'.format(test_loss))

if __name__ == "__main__":
	for epoch in range(1, args.epochs + 1):
		train(epoch)
		test(epoch)
		with torch.no_grad():
			sample = torch.randn(64, 20).to(device)
			sample = model.decode(sample).cpu()
			save_image(sample.view(64, 1, 28, 28),
					   'results/sample_' + str(epoch) + '.png')

endsnippet

